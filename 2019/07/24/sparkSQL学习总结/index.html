<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="SparkSQL,">










<meta name="description" content="SparkSQLspark2.x执行流程首先在pom里导入spark sql的依赖 12345&amp;lt;dependency&amp;gt;    &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;    &amp;lt;artifactId&amp;gt;spark-sql_2.11&amp;lt;/artifactId&amp;gt;    &amp;lt;version&amp;gt;$&amp;#123;spa">
<meta name="keywords" content="SparkSQL">
<meta property="og:type" content="article">
<meta property="og:title" content="SparkSQL学习总结">
<meta property="og:url" content="http://yoursite.com/2019/07/24/sparkSQL学习总结/index.html">
<meta property="og:site_name" content="cilibili">
<meta property="og:description" content="SparkSQLspark2.x执行流程首先在pom里导入spark sql的依赖 12345&amp;lt;dependency&amp;gt;    &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;    &amp;lt;artifactId&amp;gt;spark-sql_2.11&amp;lt;/artifactId&amp;gt;    &amp;lt;version&amp;gt;$&amp;#123;spa">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2019-08-12T11:56:47.160Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SparkSQL学习总结">
<meta name="twitter:description" content="SparkSQLspark2.x执行流程首先在pom里导入spark sql的依赖 12345&amp;lt;dependency&amp;gt;    &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;    &amp;lt;artifactId&amp;gt;spark-sql_2.11&amp;lt;/artifactId&amp;gt;    &amp;lt;version&amp;gt;$&amp;#123;spa">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/07/24/sparkSQL学习总结/">





  <title>SparkSQL学习总结 | cilibili</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">cilibili</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/24/sparkSQL学习总结/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chen Bin">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="cilibili">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">SparkSQL学习总结</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-07-24T11:31:41+08:00">
                2019-07-24
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="SparkSQL"><a href="#SparkSQL" class="headerlink" title="SparkSQL"></a>SparkSQL</h1><h2 id="spark2-x"><a href="#spark2-x" class="headerlink" title="spark2.x"></a>spark2.x</h2><h3 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a>执行流程</h3><p><strong>首先在pom里导入spark sql的依赖</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="1-创建sparkSession"><a href="#1-创建sparkSession" class="headerlink" title="1.创建sparkSession"></a>1.创建sparkSession</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val spark = SparkSession.builder()</span><br><span class="line">      .appName(&quot;SQLTest1&quot;)</span><br><span class="line">      .master(&quot;local[*]&quot;)</span><br><span class="line">      .getOrCreate()</span><br></pre></td></tr></table></figure>

<h4 id="2-创建RDD，对RDD数据整理"><a href="#2-创建RDD，对RDD数据整理" class="headerlink" title="2.创建RDD，对RDD数据整理"></a>2.创建RDD，对RDD数据整理</h4><p>方法一：用sparkContext创建RDD，再处理RDD数据</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建RDD</span></span><br><span class="line"><span class="keyword">val</span> lines: <span class="type">RDD</span>[<span class="type">String</span>] = spark.sparkContext.textFile(<span class="string">"hdfs://node-4:9000/person"</span>)</span><br><span class="line"><span class="comment">//将数据进行整理</span></span><br><span class="line"><span class="keyword">val</span> rowRDD: <span class="type">RDD</span>[<span class="type">Row</span>] = lines.map(line =&gt; &#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="type">Row</span>(id, name, age, fv)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<p><strong>方法二</strong>：用sparkSession直接创建DataSet[String] 类型，再用scalaAPI处理数据 （wordCount示例）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Dataset分布式数据集，是对RDD的进一步封装，是更加智能的RDD</span></span><br><span class="line"><span class="comment">//dataset只有一列，默认这列叫value</span></span><br><span class="line"><span class="comment">//调用sparksession.read.textFile("..")返回的是DataSet，调用ss.textFile("..")返回的是RDD</span></span><br><span class="line"><span class="keyword">val</span> lines: <span class="type">Dataset</span>[<span class="type">String</span>] = spark.read.textFile(<span class="string">"hdfs://node-4:9000/words"</span>)</span><br></pre></td></tr></table></figure>

<p>DataSet处理数据</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//整理数据(切分压平) wordCount</span></span><br><span class="line"><span class="comment">//DataSet调用了scala的API，需要导入隐式转换</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> words: <span class="type">Dataset</span>[<span class="type">String</span>] = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用DataSet的API（DSL）</span></span><br><span class="line"><span class="comment">//第一个count返回DataFrame（Transfomation），第二个count返回String（Action）</span></span><br><span class="line"><span class="keyword">val</span> r = words.groupBy($<span class="string">"value"</span> as <span class="string">"word"</span>).count().count()</span><br><span class="line"></span><br><span class="line"><span class="comment">//导入聚合函数</span></span><br><span class="line"><span class="comment">//import org.apache.spark.sql.functions._</span></span><br><span class="line"><span class="comment">//val counts = words.groupBy($"value".as("word")).agg(count("*") as "counts").orderBy($"counts" desc)</span></span><br></pre></td></tr></table></figure>

<h4 id="3-创建schema信息"><a href="#3-创建schema信息" class="headerlink" title="3.创建schema信息"></a>3.创建schema信息</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//结果类型，其实就是表头，用于描述DataFrame</span></span><br><span class="line"><span class="keyword">val</span> schema: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">List</span>(</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"id"</span>, <span class="type">LongType</span>, <span class="literal">true</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"name"</span>, <span class="type">StringType</span>, <span class="literal">true</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"age"</span>, <span class="type">IntegerType</span>, <span class="literal">true</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"fv"</span>, <span class="type">DoubleType</span>, <span class="literal">true</span>)</span><br><span class="line">))</span><br></pre></td></tr></table></figure>

<h4 id="4-创建DataFrame，关联schema信息"><a href="#4-创建DataFrame，关联schema信息" class="headerlink" title="4.创建DataFrame，关联schema信息"></a>4.创建DataFrame，关联schema信息</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建DataFrame</span></span><br><span class="line"><span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.createDataFrame(rowRDD, schema)</span><br></pre></td></tr></table></figure>

<h4 id="5-处理DataSet数据"><a href="#5-处理DataSet数据" class="headerlink" title="5.处理DataSet数据"></a>5.处理DataSet数据</h4><p>方法一：使用DataFrameAPI操作数据 （导入隐式函数）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> df2: <span class="type">Dataset</span>[<span class="type">Row</span>] = df.where($<span class="string">"fv"</span> &gt; <span class="number">98</span>).orderBy($<span class="string">"fv"</span> desc, $<span class="string">"age"</span> asc)</span><br></pre></td></tr></table></figure>

<p>方法二：使用SQL</p>
<h5 id="注册视图"><a href="#注册视图" class="headerlink" title="注册视图"></a>注册视图</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">words.createTempView(<span class="string">"v_wc"</span>)</span><br></pre></td></tr></table></figure>

<h5 id="执行SQL（Transformation，lazy）"><a href="#执行SQL（Transformation，lazy）" class="headerlink" title="执行SQL（Transformation，lazy）"></a>执行SQL（Transformation，lazy）</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">val</span> result: <span class="type">DataFrame</span> = spark.sql(</span><br><span class="line">    <span class="string">"SELECT value word, COUNT(*) counts </span></span><br><span class="line"><span class="string">    FROM v_wc </span></span><br><span class="line"><span class="string">    GROUP BY word </span></span><br><span class="line"><span class="string">    ORDER BY counts DESC"</span>)</span><br><span class="line"><span class="comment">//执行Action</span></span><br><span class="line">result.show()	</span><br><span class="line"></span><br><span class="line"><span class="comment">//往其他存储软件里写 ==================================存疑=========================</span></span><br><span class="line">result.foreachPartition(iter:(<span class="type">Iterator</span>[<span class="type">Row</span>]) =&gt;&#123;</span><br><span class="line">	&#125;)</span><br></pre></td></tr></table></figure>

<h4 id="6-关闭sparkSession连接"><a href="#6-关闭sparkSession连接" class="headerlink" title="6.关闭sparkSession连接"></a>6.关闭sparkSession连接</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="spark-join-连接表"><a href="#spark-join-连接表" class="headerlink" title="spark join 连接表"></a>spark join 连接表</h3><h4 id="1-创建两个DataFrame"><a href="#1-创建两个DataFrame" class="headerlink" title="1.创建两个DataFrame"></a>1.创建两个DataFrame</h4><ul>
<li>表1</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//对Dataset进行RDD的操作要导入隐式转换  </span></span><br><span class="line"><span class="comment">//这里是spark是SparkSession的一个实例</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> lines: <span class="type">Dataset</span>[<span class="type">String</span>] = spark.createDataset(<span class="type">List</span>(<span class="string">"1,laozhoa,china"</span>, <span class="string">"2,laoduan,usa"</span>, <span class="string">"3,laoyang,jp"</span>))</span><br><span class="line"><span class="comment">//对数据进行整理</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> tpDs: <span class="type">Dataset</span>[(<span class="type">Long</span>, <span class="type">String</span>, <span class="type">String</span>)] = lines.map(line =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> fields = line.split(<span class="string">","</span>)</span><br><span class="line">  <span class="keyword">val</span> id = fields(<span class="number">0</span>).toLong</span><br><span class="line">  <span class="keyword">val</span> name = fields(<span class="number">1</span>)</span><br><span class="line">  <span class="keyword">val</span> nationCode = fields(<span class="number">2</span>)</span><br><span class="line">  (id, name, nationCode)</span><br><span class="line">&#125;)</span><br><span class="line"><span class="keyword">val</span> df1 = tpDs.toDF(<span class="string">"id"</span>, <span class="string">"name"</span>, <span class="string">"nation"</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>表2</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">val</span> nations: <span class="type">Dataset</span>[<span class="type">String</span>] = spark.createDataset(<span class="type">List</span>(<span class="string">"china,中国"</span>, <span class="string">"usa,美国"</span>))</span><br><span class="line"><span class="comment">//对数据进行整理</span></span><br><span class="line"><span class="keyword">val</span> ndataset: <span class="type">Dataset</span>[(<span class="type">String</span>, <span class="type">String</span>)] = nations.map(l =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> fields = l.split(<span class="string">","</span>)</span><br><span class="line">  <span class="keyword">val</span> ename = fields(<span class="number">0</span>)</span><br><span class="line">  <span class="keyword">val</span> cname = fields(<span class="number">1</span>)</span><br><span class="line">  (ename, cname)</span><br><span class="line">&#125;)</span><br><span class="line"><span class="keyword">val</span> df2 = ndataset.toDF(<span class="string">"ename"</span>,<span class="string">"cname"</span>)</span><br></pre></td></tr></table></figure>

<h4 id="2-对两个DataFrame操作"><a href="#2-对两个DataFrame操作" class="headerlink" title="2.对两个DataFrame操作"></a>2.对两个DataFrame操作</h4><h5 id="方式一：SQL"><a href="#方式一：SQL" class="headerlink" title="方式一：SQL"></a>方式一：SQL</h5><p>1.创建两个视图（方式一：SQL）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df1.createTempView(<span class="string">"v_users"</span>)</span><br><span class="line">df2.createTempView(<span class="string">"v_nations"</span>)</span><br></pre></td></tr></table></figure>

<p>2.写SQL</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> r: <span class="type">DataFrame</span> = spark.sql(</span><br><span class="line">    <span class="string">"SELECT name, cname </span></span><br><span class="line"><span class="string">    FROM v_users JOIN v_nations </span></span><br><span class="line"><span class="string">    ON nation = ename"</span>)</span><br><span class="line">r.show()</span><br></pre></td></tr></table></figure>

<h5 id="方式二：DataFrameAPI"><a href="#方式二：DataFrameAPI" class="headerlink" title="方式二：DataFrameAPI"></a>方式二：DataFrameAPI</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> r = df1.join(df2, $<span class="string">"nation"</span> === $<span class="string">"ename"</span>, <span class="string">"left_outer"</span>)</span><br><span class="line">r.show()</span><br></pre></td></tr></table></figure>

<h5 id="补充：三种spark-join-方式（详见博客）"><a href="#补充：三种spark-join-方式（详见博客）" class="headerlink" title="补充：三种spark join 方式（详见博客）"></a>补充：三种spark join 方式（<a href="https://www.cnblogs.com/duodushuduokanbao/p/9911256.html" target="_blank" rel="noopener">详见博客</a>）</h5><ol>
<li><p><strong>Broadcast Hash Join</strong>（大表与小表join）(spark默认join方式)</p>
<p>步骤：</p>
<ol>
<li>broadcast阶段：将小表广播到所有的executor上，广播的算法有很多，最简单的是先发给driver，driver再统一分发给所有的executor，要不就是基于bittorrete的p2p思路；</li>
<li>hash join阶段：在每个executor上执行 hash join，小表构建为hash table，大表的分区数据匹配hash table中的数据；</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> result: <span class="type">DataFrame</span> = df1.join(df2, $<span class="string">"id"</span> === $<span class="string">"aid"</span>)</span><br><span class="line"><span class="keyword">val</span> result: <span class="type">DataFrame</span> = df1.join(broadcast(df2), $<span class="string">"id"</span> === $<span class="string">"aid"</span>)<span class="comment">//两者相同</span></span><br></pre></td></tr></table></figure>

<p>条件：</p>
<ul>
<li>被广播的表需要小于spark.sql.autoBroadcastJoinThreshold所配置的信息，默认是10M；</li>
<li>基表不能被广播，比如left outer join时，只能广播右表。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.conf.set(<span class="string">"spark.sql.autoBroadcastJoinThreshold"</span>, <span class="number">1024</span>*<span class="number">1024</span>*<span class="number">10</span>) <span class="comment">// 设置为10M</span></span><br></pre></td></tr></table></figure>

<p>先将小表在driver缓存内存，再broadCast，会对小表的扫描localTablesScan变为InMemoryScan</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df2.cache().count()</span><br><span class="line">df1.join(broadcast(df2), $<span class="string">"id"</span> === $<span class="string">"aid"</span>)</span><br></pre></td></tr></table></figure>



</li>
</ol>
<ol start="2">
<li><p><strong>Shuffle Hash Join</strong>（大表与较小表join）</p>
<p>步骤：</p>
<ol>
<li>对两张表分别按照join key进行重分区(分区函数相同的时候，相同的相同分区中的key一定是相同的)，即shuffle，目的是为了让相同join key的记录分到对应的分区中；</li>
<li>对对应分区中的数据进行join，此处先将小表分区构建为一个hash表，然后根据大表中记录的join key的hash值拿来进行匹配，即每个节点山单独执行hash算法。</li>
</ol>
<p>条件：</p>
<ul>
<li><p>分区的平均大小不超过spark.sql.autoBroadcastJoinThreshold所配置的值，默认是10M </p>
</li>
<li><p>基表不能被广播，比如left outer join时，只能广播右表</p>
</li>
<li><p>一侧的表要明显小于另外一侧，小的一侧将被广播（明显小于的定义为3倍小，此处为经验值）</p>
</li>
</ul>
</li>
<li><p><strong>Sort Merge Join</strong>（两个大表进行join）</p>
<p>步骤：</p>
<ol>
<li><p>shuffle阶段：将两张大表根据join key进行重新分区，两张表数据会分布到整个集群，以便分布式并行处理；</p>
</li>
<li><p>sort阶段：对单个分区节点的两表数据，分别进行排序；</p>
</li>
<li><p>merge阶段：对排好序的两张分区表数据执行join操作。join操作很简单，分别遍历两个有序序列，碰到相同join key就merge输出，否则取更小一边</p>
</li>
</ol>
<p>条件：</p>
<ul>
<li>两个大表</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.conf.set(<span class="string">"spark.sql.join.preferSortMergeJoin"</span>, <span class="literal">true</span>)<span class="comment">//方法一</span></span><br><span class="line">spark.sql.autoBroadcastJoinThreshold=<span class="number">-1</span><span class="comment">//方法二</span></span><br></pre></td></tr></table></figure>



</li>
</ol>
<hr>
<h3 id="自定义函数-UDF、UDAF"><a href="#自定义函数-UDF、UDAF" class="headerlink" title="自定义函数 UDF、UDAF"></a>自定义函数 UDF、UDAF</h3><h4 id="UDF-（user-defined-function-1映射到1）"><a href="#UDF-（user-defined-function-1映射到1）" class="headerlink" title="UDF （user-defined function 1映射到1）"></a>UDF （user-defined function 1映射到1）</h4><ol>
<li>自定义函数并注册</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//该函数的功能是（输入一个IP地址对应的十进制，返回一个省份名称）</span></span><br><span class="line">spark.udf.register(<span class="string">"ip2Province"</span>, (ipNum: <span class="type">Long</span>) =&gt; &#123; <span class="comment">//传入IP:Long数据</span></span><br><span class="line">      ...<span class="comment">//处理逻辑见 “统计IP示例”</span></span><br><span class="line">      province <span class="comment">//返回省份字符串</span></span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>调用</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">"SELECT ip2Province(ip_num) province FROM v_log "</span>).show()</span><br></pre></td></tr></table></figure>

<h4 id="UDAF-（user-defined-aggregate-function-n映射到1）"><a href="#UDAF-（user-defined-aggregate-function-n映射到1）" class="headerlink" title="UDAF （user-defined-aggregate function n映射到1）"></a>UDAF （user-defined-aggregate function n映射到1）</h4><h5 id="1-定义聚合函数"><a href="#1-定义聚合函数" class="headerlink" title="1. 定义聚合函数"></a>1. 定义聚合函数</h5><p>$$<br>函数功能：   \sqrt[n]{x_1<em>x_2</em>…*x_n}<br>$$</p>
<ul>
<li>继承UserDefinedAggregateFunction，实现8个方法</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GeoMean</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span></span></span><br></pre></td></tr></table></figure>

<ul>
<li>输入数据的类型</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">List</span>(</span><br><span class="line">  <span class="type">StructField</span>(<span class="string">"value"</span>, <span class="type">DoubleType</span>)</span><br><span class="line">))</span><br></pre></td></tr></table></figure>

<ul>
<li>产生中间结果的数据类型</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">List</span>(</span><br><span class="line">  <span class="comment">//相乘之后返回的积</span></span><br><span class="line">  <span class="type">StructField</span>(<span class="string">"product"</span>, <span class="type">DoubleType</span>),</span><br><span class="line">  <span class="comment">//参与运算数字的个数</span></span><br><span class="line">  <span class="type">StructField</span>(<span class="string">"counts"</span>, <span class="type">LongType</span>)</span><br><span class="line">))</span><br></pre></td></tr></table></figure>

<ul>
<li>最终返回的结果类型</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">DoubleType</span></span><br></pre></td></tr></table></figure>

<ul>
<li>确保一致性 一般用true</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<ul>
<li>指定初始值</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">//相乘的初始值</span></span><br><span class="line">  buffer(<span class="number">0</span>) = <span class="number">1.0</span></span><br><span class="line">  <span class="comment">//参与运算数字的个数的初始值</span></span><br><span class="line">  buffer(<span class="number">1</span>) = <span class="number">0</span>L</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>每有一条数据参与运算就更新一下中间结果(update相当于在每一个分区中的运算)</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">//每有一个数字参与运算就进行相乘（包含中间结果）</span></span><br><span class="line">  buffer(<span class="number">0</span>) = buffer.getDouble(<span class="number">0</span>) * input.getDouble(<span class="number">0</span>)</span><br><span class="line">  <span class="comment">//参与运算数据的个数也有更新</span></span><br><span class="line">  buffer(<span class="number">1</span>) = buffer.getLong(<span class="number">1</span>) + <span class="number">1</span>L</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>全局聚合</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">//每个分区计算的结果进行相乘</span></span><br><span class="line">  buffer1(<span class="number">0</span>) =  buffer1.getDouble(<span class="number">0</span>) * buffer2.getDouble(<span class="number">0</span>)</span><br><span class="line">  <span class="comment">//每个分区参与预算的中间结果进行相加</span></span><br><span class="line">  buffer1(<span class="number">1</span>) =  buffer1.getLong(<span class="number">1</span>) + buffer2.getLong(<span class="number">1</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>计算最终的结果</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Double</span> = &#123;</span><br><span class="line">  math.pow(buffer.getDouble(<span class="number">0</span>), <span class="number">1.</span>toDouble / buffer.getLong(<span class="number">1</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="2-调用聚合函数"><a href="#2-调用聚合函数" class="headerlink" title="2. 调用聚合函数"></a>2. 调用聚合函数</h5><ol>
<li>使用SQL</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> geomean = <span class="keyword">new</span> <span class="type">GeoMean</span></span><br><span class="line"><span class="keyword">val</span> range: <span class="type">Dataset</span>[<span class="type">Long</span>] = spark.range(<span class="number">1</span>, <span class="number">11</span>)<span class="comment">//定义1~11的单列数据</span></span><br><span class="line"><span class="comment">//注册函数</span></span><br><span class="line">spark.udf.register(<span class="string">"gm"</span>, geomean)</span><br><span class="line"><span class="comment">//将range这个Dataset[Long]注册成视图</span></span><br><span class="line">range.createTempView(<span class="string">"v_range"</span>)</span><br><span class="line"><span class="keyword">val</span> result = spark.sql(<span class="string">"SELECT gm(id) result FROM v_range"</span>)<span class="comment">//调用聚合函数</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li>DSL风格</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//使用DSL风格不用注册和建视图</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> result = range.groupBy().agg(geomean($<span class="string">"id"</span>).as(<span class="string">"geomean"</span>)) <span class="comment">//这里可以不用掉groupBy</span></span><br></pre></td></tr></table></figure>

<h4 id="UDTF"><a href="#UDTF" class="headerlink" title="UDTF"></a>UDTF</h4><p>输入一行，返回多行（hive）一对多    spark SQL中没有UDTF，spark中用flatMap即可实现该功能</p>
<h3 id="统计IP地址示例"><a href="#统计IP地址示例" class="headerlink" title="统计IP地址示例"></a>统计IP地址示例</h3><h4 id="初级版本：连接两张表查询"><a href="#初级版本：连接两张表查询" class="headerlink" title="初级版本：连接两张表查询"></a>初级版本：连接两张表查询</h4><ol>
<li>创建sparkSession</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">"JoinTest"</span>)</span><br><span class="line">      .master(<span class="string">"local[*]"</span>)</span><br><span class="line">      .getOrCreate()</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>创建”IP-地区规则表”&amp;”IP记录表”的DataFrame</li>
</ol>
<ul>
<li>IP-地区规则表</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//取到HDFS中的ip规则</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._                                   <span class="comment">//导入隐式参数因为用了Map方法</span></span><br><span class="line"><span class="keyword">val</span> rulesLines:<span class="type">Dataset</span>[<span class="type">String</span>] = spark.read.textFile(args(<span class="number">0</span>))</span><br><span class="line"><span class="comment">//整理ip规则数据()</span></span><br><span class="line"><span class="keyword">val</span> ruleDataFrame: <span class="type">DataFrame</span> = rulesLines.map(line =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> fields = line.split(<span class="string">"[|]"</span>)</span><br><span class="line">  <span class="keyword">val</span> startNum = fields(<span class="number">2</span>).toLong</span><br><span class="line">  <span class="keyword">val</span> endNum = fields(<span class="number">3</span>).toLong</span><br><span class="line">  <span class="keyword">val</span> province = fields(<span class="number">6</span>)</span><br><span class="line">  (startNum, endNum, province)</span><br><span class="line">&#125;).toDF(<span class="string">"snum"</span>, <span class="string">"enum"</span>, <span class="string">"province"</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>IP记录表</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建RDD，读取ip访问日志</span></span><br><span class="line"><span class="keyword">val</span> accessLines: <span class="type">Dataset</span>[<span class="type">String</span>] = spark.read.textFile(args(<span class="number">1</span>))</span><br><span class="line"><span class="comment">//整理数据</span></span><br><span class="line"><span class="keyword">val</span> ipDataFrame: <span class="type">DataFrame</span> = accessLines.map(log =&gt; &#123;</span><br><span class="line">  <span class="comment">//将log日志的每一行进行切分</span></span><br><span class="line">  <span class="keyword">val</span> fields = log.split(<span class="string">"[|]"</span>)</span><br><span class="line">  <span class="keyword">val</span> ip = fields(<span class="number">1</span>)</span><br><span class="line">  <span class="comment">//将ip转换成十进制</span></span><br><span class="line">  <span class="keyword">val</span> ipNum = <span class="type">MyUtils</span>.ip2Long(ip) <span class="comment">//将111.11.11.11整理成Long型数据</span></span><br><span class="line">  ipNum</span><br><span class="line">&#125;).toDF(<span class="string">"ip_num"</span>)</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>创建两张视图</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ruleDataFrame.createTempView(<span class="string">"v_rules"</span>)</span><br><span class="line">ipDataFrame.createTempView(<span class="string">"v_ips"</span>)</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>执行SQL</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> r = spark.sql(</span><br><span class="line"><span class="string">"SELECT province, count(*) counts </span></span><br><span class="line"><span class="string">FROM v_ips JOIN v_rules </span></span><br><span class="line"><span class="string">ON (ip_num &gt;= snum AND ip_num &lt;= enum)         </span></span><br><span class="line"><span class="string">GROUP BY province ORDER BY counts DESC"</span>        </span><br><span class="line">).show()										<span class="comment">//两表字段非等值内连接</span></span><br></pre></td></tr></table></figure>

<ol start="5">
<li>关闭数据流</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.stop</span><br></pre></td></tr></table></figure>

<h4 id="高级版本：缓存广播小表-UDF"><a href="#高级版本：缓存广播小表-UDF" class="headerlink" title="高级版本：缓存广播小表,UDF"></a>高级版本：缓存广播小表,UDF</h4><ol>
<li><p>创建sparkSession</p>
</li>
<li><p>整理IP规则为Array[(Long, Long, String)]，不再创建表了</p>
</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> rulesLines:<span class="type">Dataset</span>[<span class="type">String</span>] = spark.read.textFile(args(<span class="number">0</span>))</span><br><span class="line"><span class="comment">//整理ip规则数据()</span></span><br><span class="line"><span class="keyword">val</span> rluesDataset = rulesLines.map(line =&gt; &#123;</span><br><span class="line">	...</span><br><span class="line">    (startNum, endNum, province)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>收集ip规则到Driver端，并且广播</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rulesInDriver: <span class="type">Array</span>[(<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">String</span>)] = rluesDataset.collect() <span class="comment">//已缓存内存</span></span><br><span class="line"><span class="comment">//广播(必须使用sparkcontext)</span></span><br><span class="line"><span class="comment">//将广播变量的引用返回到Driver端</span></span><br><span class="line"><span class="keyword">val</span> broadcastRef: <span class="type">Broadcast</span>[<span class="type">Array</span>[(<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">String</span>)]] = spark.sparkContext.broadcast(rulesInDriver)</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>读IP记录创建dataFrame和视图</li>
<li>定义一个自定义函数（UDF），并注册</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//该函数的功能是（输入一个IP地址对应的十进制，返回一个省份名称）</span></span><br><span class="line">spark.udf.register(<span class="string">"ip2Province"</span>, (ipNum: <span class="type">Long</span>) =&gt; &#123;</span><br><span class="line">  <span class="comment">//查找ip规则（事先已经广播了，已经在Executor中了）</span></span><br><span class="line">  <span class="comment">//函数的逻辑是在Executor中执行的，怎样获取ip规则的对应的数据呢？</span></span><br><span class="line">  <span class="comment">//使用广播变量的引用，就可以获得</span></span><br><span class="line">  <span class="keyword">val</span> ipRulesInExecutor: <span class="type">Array</span>[(<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">String</span>)] = broadcastRef.value</span><br><span class="line">  <span class="comment">//根据IP地址对应的十进制查找省份名称</span></span><br><span class="line">  <span class="keyword">val</span> index = <span class="type">MyUtils</span>.binarySearch(ipRulesInExecutor, ipNum)</span><br><span class="line">  <span class="keyword">var</span> province = <span class="string">"未知"</span></span><br><span class="line">  <span class="keyword">if</span>(index != <span class="number">-1</span>) &#123;</span><br><span class="line">    province = ipRulesInExecutor(index)._3</span><br><span class="line">  &#125;</span><br><span class="line">  province</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<p>6.SQL里调用自定义函数</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">    <span class="string">"SELECT ip2Province(ip_num) province, COUNT(*) counts </span></span><br><span class="line"><span class="string">	FROM v_log </span></span><br><span class="line"><span class="string">	GROUP BY province </span></span><br><span class="line"><span class="string">	ORDER BY counts DESC"</span>).show()</span><br></pre></td></tr></table></figure>

<h3 id="最受欢迎老师TOPN"><a href="#最受欢迎老师TOPN" class="headerlink" title="最受欢迎老师TOPN"></a>最受欢迎老师TOPN</h3><h4 id="hive窗口分析函数"><a href="#hive窗口分析函数" class="headerlink" title="hive窗口分析函数"></a>hive窗口分析函数</h4><ul>
<li><p>row_number() over(partition by subject order by counts desc)    增加行号</p>
</li>
<li><p>rank() over(order by counts desc)       增加序列号（1，2，2，4…）</p>
</li>
<li><p>dense_rank() over(order by counts desc)         增加序列号（1，2，2，3….）</p>
</li>
</ul>
<h4 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h4><ol>
<li>获取sparkSessioni</li>
<li>创建dataFrame   (subject, teacher).toDF(“subject”, “teacher”)</li>
<li>创建视图.createTempView(“v_sub_teacher”)</li>
<li><strong>SQL创建DataFrame</strong>（根据v_sub_teacher表创建多出counts列的新表）</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//SQL创建新的DataFrame，该学科下的老师的访问次数</span></span><br><span class="line"><span class="keyword">val</span> temp1: <span class="type">DataFrame</span> = spark.sql(</span><br><span class="line"><span class="string">"SELECT subject, teacher, count(*) counts</span></span><br><span class="line"><span class="string">FROM v_sub_teacher </span></span><br><span class="line"><span class="string">GROUP BY subject, teacher"</span>)</span><br><span class="line"><span class="comment">//创建新视图</span></span><br><span class="line">temp1.createTempView(<span class="string">"v_temp_sub_teacher_counts"</span>)</span><br></pre></td></tr></table></figure>

<ol start="5">
<li>对新视图执行SQL</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//scala用法：字符串前加s，""内直接用$取参数值</span></span><br><span class="line"><span class="keyword">val</span> temp2 = spark.sql(s</span><br><span class="line"><span class="string">"SELECT *, row_number() over(order by counts desc) g_rk </span></span><br><span class="line"><span class="string">FROM </span></span><br><span class="line"><span class="string">(SELECT subject, teacher, counts, </span></span><br><span class="line"><span class="string">row_number() over(partition by subject order by counts desc) sub_rk</span></span><br><span class="line"><span class="string">FROM v_temp_sub_teacher_counts) temp2 </span></span><br><span class="line"><span class="string">WHERE sub_rk &lt;= $topN"</span>)  <span class="comment">//取出前topN的数据</span></span><br></pre></td></tr></table></figure>

<h3 id="读存多种数据源"><a href="#读存多种数据源" class="headerlink" title="读存多种数据源"></a>读存多种数据源</h3><h4 id="JDBC"><a href="#JDBC" class="headerlink" title="JDBC"></a>JDBC</h4><p><strong>读取数据</strong></p>
<ol>
<li>定义Property对象保存连接信息</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> props = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">props.put(<span class="string">"user"</span>,<span class="string">"root"</span>)</span><br><span class="line">props.put(<span class="string">"password"</span>,<span class="string">"root"</span>)</span><br><span class="line">props.put(<span class="string">"driver"</span>,<span class="string">"com.mysql.jdbc.Driver"</span>) <span class="comment">//导入mysql-connector依赖</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li>调用访问jdbc依赖</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data:<span class="type">DataFrame</span> = spark.read.jdbc(<span class="string">"jdbc:mysql://localhost:3306/fe"</span>,<span class="string">"job"</span>,props)</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>打印表信息</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data.printSchema() <span class="comment">//表头信息</span></span><br><span class="line">data.show() <span class="comment">//表信息</span></span><br></pre></td></tr></table></figure>

<p><strong>写入数据</strong></p>
<p><strong>spark.write.mode(“xx”)类型：</strong>Append、ErrorIfExists、Ignore、OverWrite</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> props = <span class="keyword">new</span> <span class="type">Properties</span>() <span class="comment">//写入数据不用jdbcDriver</span></span><br><span class="line">props.put(<span class="string">"user"</span>,<span class="string">"root"</span>)</span><br><span class="line">props.put(<span class="string">"password"</span>,<span class="string">"root"</span>)</span><br><span class="line">reslut.write.mode(<span class="string">"ignore"</span>).jdbc(</span><br><span class="line">    <span class="string">"jdbc:mysql://localhost:3306/bigdata"</span>, <span class="string">"logs1"</span>, props)</span><br></pre></td></tr></table></figure>

<p><strong>备注</strong>：写入text注意</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//DataFrame保存成text时(只能保存一列String类型的数据)!!!!</span></span><br><span class="line">reslut.write.text(<span class="string">"/Users/zx/Desktop/text"</span>)</span><br></pre></td></tr></table></figure>

<h4 id="JSON"><a href="#JSON" class="headerlink" title="JSON"></a>JSON</h4><p><strong>读取数据</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> jsons: <span class="type">DataFrame</span> = spark.read.json(<span class="string">"/Users/zx/Desktop/json"</span>)</span><br></pre></td></tr></table></figure>

<p><strong>写入数据</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reslut.write.json(<span class="string">"/Users/zx/Desktop/json"</span>)</span><br></pre></td></tr></table></figure>

<h4 id="CSV"><a href="#CSV" class="headerlink" title="CSV"></a>CSV</h4><p><strong>读取数据</strong></p>
<p>读取出的数据scehma信息为_c1,_c2…_cn  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> csv: <span class="type">DataFrame</span> = spark.read.csv(<span class="string">"/Users/zx/Desktop/csv"</span>)</span><br></pre></td></tr></table></figure>

<p><strong>存入数据</strong></p>
<p>存入数据没有表头信息</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reslut.write.csv(<span class="string">"/Users/zx/Desktop/csv"</span>)</span><br></pre></td></tr></table></figure>

<h4 id="Parquet"><a href="#Parquet" class="headerlink" title="Parquet"></a>Parquet</h4><p>Parquet：面向分析型业务的列式存储格式、spark可以只读取其中某几列</p>
<p><strong>读取数据</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> parquetLine: <span class="type">DataFrame</span> = spark.read.parquet(<span class="string">"/Users/zx/Desktop/parquet"</span>)</span><br></pre></td></tr></table></figure>

<p><strong>写入数据</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reslut.write.parquet(<span class="string">"hdfs://node-4:9000/parquet"</span>)</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="sparkSQL整合hive"><a href="#sparkSQL整合hive" class="headerlink" title="sparkSQL整合hive"></a>sparkSQL整合hive</h3><p>跟hive没太的关系，就是使用了hive的标准（HQL， 元数据库、UDF、序列化、反序列化机制</p>
<p>真正要计算的数据是保存在HDFS中，mysql这个元数据库，保存的是hive表的描述信息，描述了有哪些database、table、以及表有多少列，每一列是什么类型，还要描述表的数据保存在hdfs的什么位置</p>
<p><strong>hive的元数据库的功能：</strong><br>    建立了一种映射关系，执行HQL时，先到MySQL元数据库中查找描述信息，然后根据描述信息生成任务，然后将任务下发到spark集群中执行</p>
<p><strong>pom中导入spark-hive依赖</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="方式一：spark-sql-shell-配置启动"><a href="#方式一：spark-sql-shell-配置启动" class="headerlink" title="方式一：spark-sql shell 配置启动"></a>方式一：spark-sql shell 配置启动</h4><ol>
<li>安装MySQL（hive的元数据库）并创建一个普通用户，并且授权</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. CREATE USER &apos;xiaoniu&apos;@&apos;%&apos; IDENTIFIED BY &apos;123568&apos;; </span><br><span class="line">2. GRANT ALL PRIVILEGES ON hivedb.* TO &apos;xiaoniu&apos;@&apos;%&apos; IDENTIFIED BY &apos;123568&apos; </span><br><span class="line">WITH GRANT OPTION;</span><br><span class="line">3. FLUSH PRIVILEGES;</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>在spark的conf目录下添加一个hive-site.xml（创建一个hive的配置文件）</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://node-6:3306/hivedb?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">   <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>bigdata<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>123568<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ol start="3">
<li><p>上传一个mysql连接驱动（sparkSubmit也要连接MySQL，获取元数据信息）</p>
<p>启动spark-sql shell 命令：（如果mysql驱动在spark/lib下就不用跟driver-class-path）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./spark-sql --master spark://node-4:7077,node-5:7077 --driver-class-path /home/xiaoniu/mysql-connector-java-5.1.7-bin.jar</span><br></pre></td></tr></table></figure>

<p>spark-sql 执行 SQL文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>spark sql 整合hive，将hive的sql写在一个文件中执行（用-f这个参数）</span><br><span class="line">spark-sql --master spark://node-4:7077,node-5:7077 -f hive-sqls.sql</span><br></pre></td></tr></table></figure>
</li>
<li><p>sparkSQL会在mysql上创建一个database(hivedb保存元数据信息)</p>
<p><strong>注意！：</strong><u>需要手动改一下</u>DBS表中的DB_LOCATION_UIR(由本地地址file:/)改成hdfs的地址(hdfs://master:9000/xx)</p>
<p><strong>补充</strong>：因为在建表和导入本地数据时虽然可以成功，但spark-sql shell在执行查询等语句时是多台机器在本地查找数据计算，只有本地的机器有数据，其他机器找不到数据从而出错。</p>
</li>
<li><p>要在/etc/profile中配置一个环节变量(让sparkSQL知道hdfs在哪里，其实就是namenode在哪里)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">exprot HADOOP_CONF_DIR=$HADOOP_HOME #指定HADOOP配置文件路径</span><br></pre></td></tr></table></figure>
</li>
<li><p>重新启动SparkSQL的命令行</p>
</li>
</ol>
<h4 id="方式二：IDE中执行"><a href="#方式二：IDE中执行" class="headerlink" title="方式二：IDE中执行"></a>方式二：IDE中执行</h4><ol>
<li>开启spark对hive的支持</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">SparkSession</span>.builder().enableHiveSupport()....</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><p>在resource文件夹下添加hive-site.xml<strong>（非高可用）</strong></p>
<p>在本地运行需要元数据库配置文件信息，如果在集群运行就不需要</p>
<p><strong>备注</strong>：如果配置了高可用hdfs（两个namenode，DBS表中的DB_LOCATION_UIR的hdfs路径例如配置为hdfs://ns/xxx，提交运行时找不到hdfs://ns地址，如果是hdfs://master:9000就可以），则还需要添加core-site.xml、hdfs-site.xml</p>
</li>
<li><p>实行sparkSQL（执行HIVE）</p>
</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> result: <span class="type">DataFrame</span> = spark.sql(<span class="string">"SELECT * FROM t_boy ORDER BY fv DESC"</span>)</span><br><span class="line"><span class="keyword">val</span> sql: <span class="type">DataFrame</span> = spark.sql(<span class="string">"CREATE TABLE niu (id bigint, name string)"</span>)</span><br><span class="line">result.show()</span><br><span class="line">sql.show</span><br></pre></td></tr></table></figure>

<h4 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h4><ul>
<li>非高可用：</li>
</ul>
<p>core-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://master:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>hdfs-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/root/hdpdata/name/<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/root/hdpdata/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>slave1:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li><p>高可用：</p>
<p>见文件：<a href="sparkSQL学习总结/conf/core-site.xml">core-site.xml</a>、<a href="sparkSQL学习总结/conf/hdfs-site.xml">hdfs-site.xml</a></p>
</li>
</ul>
<h2 id="spark1-x"><a href="#spark1-x" class="headerlink" title="spark1.x"></a>spark1.x</h2><h3 id="创建方式一"><a href="#创建方式一" class="headerlink" title="创建方式一"></a>创建方式一</h3><p>1.创建SparkContext</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//提交的这个程序可以连接到Spark集群中</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"SQLDemo1"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line"><span class="comment">//创建SparkSQL的连接（程序执行的入口）</span></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br></pre></td></tr></table></figure>

<p>2.创建SQLContext</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//sparkContext不能创建特殊的RDD（DataFrame）</span></span><br><span class="line"><span class="comment">//将SparkContext包装进而增强</span></span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br></pre></td></tr></table></figure>

<p>3.创建一个类（建议case class不用new和序列化），并定义类的成员变量</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Boy</span>(<span class="params">id: <span class="type">Long</span>, name: <span class="type">String</span>, age: <span class="type">Int</span>, fv: <span class="type">Double</span></span>)</span></span><br></pre></td></tr></table></figure>

<p>4.创建RDD,将数据进行整理成class对象（关联class，将非结构化数据转换成结构化数据）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建特殊的RDD（DataFrame），就是有schema信息的RDD</span></span><br><span class="line"><span class="comment">//先有一个普通的RDD，然后在关联上schema，进而转成DataFrame</span></span><br><span class="line"><span class="keyword">val</span> lines = sc.textFile(<span class="string">"hdfs://node-4:9000/person"</span>)</span><br><span class="line"><span class="comment">//将数据进行整理</span></span><br><span class="line"><span class="keyword">val</span> boyRDD: <span class="type">RDD</span>[<span class="type">Boy</span>] = lines.map(line =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> fields = line.split(<span class="string">","</span>)</span><br><span class="line">    <span class="keyword">val</span> id = fields(<span class="number">0</span>).toLong</span><br><span class="line">	...</span><br><span class="line">    <span class="type">Boy</span>(id, name, age, fv)</span><br><span class="line">&#125;)</span><br><span class="line"><span class="comment">//该RDD装的是Boy类型的数据，有了shcma信息，但是还是一个RDD</span></span><br></pre></td></tr></table></figure>

<p>6.将RDD转换成DataFrame（导入隐式转换），显式调用toDF方法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//将RDD转换成DataFrame</span></span><br><span class="line"><span class="comment">//导入隐式转换</span></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"><span class="keyword">val</span> bdf: <span class="type">DataFrame</span> = boyRDD.toDF</span><br><span class="line"><span class="comment">//变成DF后就可以使用两种API进行编程了</span></span><br></pre></td></tr></table></figure>

<p>7.将DataFrame注册成临时表（SQL方法）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//把DataFrame先注册临时表</span></span><br><span class="line">bdf.registerTempTable(<span class="string">"t_boy"</span>)</span><br></pre></td></tr></table></figure>

<p>8.书写SQL（Transformation）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//书写SQL（SQL方法应其实是Transformation）</span></span><br><span class="line"><span class="keyword">val</span> result: <span class="type">DataFrame</span> = sqlContext.sql(<span class="string">"SELECT * FROM t_boy ORDER BY fv desc, age asc"</span>)</span><br></pre></td></tr></table></figure>

<p>9.执行Action</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">result.show()</span><br><span class="line">sc.stop()</span><br></pre></td></tr></table></figure>

<h3 id="创建方式二"><a href="#创建方式二" class="headerlink" title="创建方式二"></a>创建方式二</h3><p>1.创建SparkContext<br>2.创建SQLContext<br>3.创建RDD，将数据整理成Row对象</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rowRDD: <span class="type">RDD</span>[<span class="type">Row</span>] = lines.map(line =&gt; &#123;</span><br><span class="line">      ...</span><br><span class="line">      <span class="type">Row</span>(id, name, age, fv)</span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure>

<p>4.创建StructType（schema）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//结果类型，其实就是表头，用于描述DataFrame</span></span><br><span class="line"><span class="keyword">val</span> sch: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">List</span>(</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"id"</span>, <span class="type">LongType</span>, <span class="literal">true</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"name"</span>, <span class="type">StringType</span>, <span class="literal">true</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"age"</span>, <span class="type">IntegerType</span>, <span class="literal">true</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"fv"</span>, <span class="type">DoubleType</span>, <span class="literal">true</span>)</span><br><span class="line">))</span><br></pre></td></tr></table></figure>

<p>5.通过rowRDD和schema创建DataFrame （使用sqlContext方法，不用导入隐式转换）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//将RowRDD关联schem</span></span><br><span class="line"><span class="keyword">val</span> bdf: <span class="type">DataFrame</span> = sqlContext.createDataFrame(rowRDD, sch)</span><br></pre></td></tr></table></figure>

<p>6.使用DSL风格操作数据（dataFrameAPI操作数据）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//不使用SQL的方式，就不用注册临时表了</span></span><br><span class="line"><span class="keyword">val</span> df1: <span class="type">DataFrame</span> = bdf.select(<span class="string">"name"</span>, <span class="string">"age"</span>, <span class="string">"fv"</span>)</span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"><span class="keyword">val</span> df2: <span class="type">DataFrame</span> = df1.orderBy($<span class="string">"fv"</span> desc, $<span class="string">"age"</span> asc)</span><br></pre></td></tr></table></figure>

<p>//6.将DataFrame注册成临时表（SQL方式）<br>//7.书写SQL（Transformation）<br>8.执行Action</p>
<h1 id="导入隐式转换的地方："><a href="#导入隐式转换的地方：" class="headerlink" title="导入隐式转换的地方："></a>导入隐式转换的地方：</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"><span class="keyword">val</span> bdf: <span class="type">DataFrame</span> = boyRDD.toDF</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sparkSession.implicits._</span><br><span class="line"><span class="keyword">val</span> df2: <span class="type">Dataset</span>[<span class="type">Row</span>] = df.where($<span class="string">"fv"</span> &gt; <span class="number">98</span>).orderBy($<span class="string">"fv"</span> desc, $<span class="string">"age"</span> asc)</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> lines: <span class="type">Dataset</span>[<span class="type">String</span>] = spark.createDataset(<span class="type">List</span>(<span class="string">"1,laozhoa,china"</span>, <span class="string">"2,laoduan,usa"</span>, <span class="string">"3,laoyang,jp"</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> range: <span class="type">Dataset</span>[<span class="type">Long</span>] = spark.range(<span class="number">1</span>, <span class="number">11</span>)</span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> result = range.groupBy().agg(geomean($<span class="string">"id"</span>).as(<span class="string">"geomean"</span>)) <span class="comment">//这里可以不用掉groupBy</span></span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._ <span class="comment">//dataSet调用map方法用到了隐式转换</span></span><br><span class="line"><span class="keyword">val</span> rulesLines:<span class="type">Dataset</span>[<span class="type">String</span>] = spark.read.textFile(args(<span class="number">0</span>))</span><br><span class="line"><span class="comment">//整理ip规则数据()</span></span><br><span class="line"><span class="keyword">val</span> ruleDataFrame: <span class="type">DataFrame</span> = rulesLines.map(line =&gt; &#123;...&#125;)</span><br></pre></td></tr></table></figure>


      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/SparkSQL/" rel="tag"># SparkSQL</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/07/24/sparkRDD学习总结/" rel="next" title="SparkRDD学习总结">
                <i class="fa fa-chevron-left"></i> SparkRDD学习总结
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/07/24/sparkStreaming笔记/" rel="prev" title="SparkStreaming学习总结">
                SparkStreaming学习总结 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Chen Bin</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">12</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">categories</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">15</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/cilibili" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#SparkSQL"><span class="nav-number">1.</span> <span class="nav-text">SparkSQL</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#spark2-x"><span class="nav-number">1.1.</span> <span class="nav-text">spark2.x</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#执行流程"><span class="nav-number">1.1.1.</span> <span class="nav-text">执行流程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-创建sparkSession"><span class="nav-number">1.1.1.1.</span> <span class="nav-text">1.创建sparkSession</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-创建RDD，对RDD数据整理"><span class="nav-number">1.1.1.2.</span> <span class="nav-text">2.创建RDD，对RDD数据整理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-创建schema信息"><span class="nav-number">1.1.1.3.</span> <span class="nav-text">3.创建schema信息</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-创建DataFrame，关联schema信息"><span class="nav-number">1.1.1.4.</span> <span class="nav-text">4.创建DataFrame，关联schema信息</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-处理DataSet数据"><span class="nav-number">1.1.1.5.</span> <span class="nav-text">5.处理DataSet数据</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#注册视图"><span class="nav-number">1.1.1.5.1.</span> <span class="nav-text">注册视图</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#执行SQL（Transformation，lazy）"><span class="nav-number">1.1.1.5.2.</span> <span class="nav-text">执行SQL（Transformation，lazy）</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-关闭sparkSession连接"><span class="nav-number">1.1.1.6.</span> <span class="nav-text">6.关闭sparkSession连接</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#spark-join-连接表"><span class="nav-number">1.1.2.</span> <span class="nav-text">spark join 连接表</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-创建两个DataFrame"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">1.创建两个DataFrame</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-对两个DataFrame操作"><span class="nav-number">1.1.2.2.</span> <span class="nav-text">2.对两个DataFrame操作</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#方式一：SQL"><span class="nav-number">1.1.2.2.1.</span> <span class="nav-text">方式一：SQL</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#方式二：DataFrameAPI"><span class="nav-number">1.1.2.2.2.</span> <span class="nav-text">方式二：DataFrameAPI</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#补充：三种spark-join-方式（详见博客）"><span class="nav-number">1.1.2.2.3.</span> <span class="nav-text">补充：三种spark join 方式（详见博客）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#自定义函数-UDF、UDAF"><span class="nav-number">1.1.3.</span> <span class="nav-text">自定义函数 UDF、UDAF</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#UDF-（user-defined-function-1映射到1）"><span class="nav-number">1.1.3.1.</span> <span class="nav-text">UDF （user-defined function 1映射到1）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#UDAF-（user-defined-aggregate-function-n映射到1）"><span class="nav-number">1.1.3.2.</span> <span class="nav-text">UDAF （user-defined-aggregate function n映射到1）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-定义聚合函数"><span class="nav-number">1.1.3.2.1.</span> <span class="nav-text">1. 定义聚合函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-调用聚合函数"><span class="nav-number">1.1.3.2.2.</span> <span class="nav-text">2. 调用聚合函数</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#UDTF"><span class="nav-number">1.1.3.3.</span> <span class="nav-text">UDTF</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#统计IP地址示例"><span class="nav-number">1.1.4.</span> <span class="nav-text">统计IP地址示例</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#初级版本：连接两张表查询"><span class="nav-number">1.1.4.1.</span> <span class="nav-text">初级版本：连接两张表查询</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#高级版本：缓存广播小表-UDF"><span class="nav-number">1.1.4.2.</span> <span class="nav-text">高级版本：缓存广播小表,UDF</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#最受欢迎老师TOPN"><span class="nav-number">1.1.5.</span> <span class="nav-text">最受欢迎老师TOPN</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#hive窗口分析函数"><span class="nav-number">1.1.5.1.</span> <span class="nav-text">hive窗口分析函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#流程"><span class="nav-number">1.1.5.2.</span> <span class="nav-text">流程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#读存多种数据源"><span class="nav-number">1.1.6.</span> <span class="nav-text">读存多种数据源</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#JDBC"><span class="nav-number">1.1.6.1.</span> <span class="nav-text">JDBC</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#JSON"><span class="nav-number">1.1.6.2.</span> <span class="nav-text">JSON</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CSV"><span class="nav-number">1.1.6.3.</span> <span class="nav-text">CSV</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Parquet"><span class="nav-number">1.1.6.4.</span> <span class="nav-text">Parquet</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sparkSQL整合hive"><span class="nav-number">1.1.7.</span> <span class="nav-text">sparkSQL整合hive</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#方式一：spark-sql-shell-配置启动"><span class="nav-number">1.1.7.1.</span> <span class="nav-text">方式一：spark-sql shell 配置启动</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#方式二：IDE中执行"><span class="nav-number">1.1.7.2.</span> <span class="nav-text">方式二：IDE中执行</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#配置文件"><span class="nav-number">1.1.7.3.</span> <span class="nav-text">配置文件</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#spark1-x"><span class="nav-number">1.2.</span> <span class="nav-text">spark1.x</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#创建方式一"><span class="nav-number">1.2.1.</span> <span class="nav-text">创建方式一</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#创建方式二"><span class="nav-number">1.2.2.</span> <span class="nav-text">创建方式二</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#导入隐式转换的地方："><span class="nav-number">2.</span> <span class="nav-text">导入隐式转换的地方：</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chen Bin</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
