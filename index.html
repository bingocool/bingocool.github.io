<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="cilibili">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="cilibili">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="cilibili">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/">





  <title>cilibili</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">cilibili</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/24/SPARK学习笔记/sparkStreaming笔记/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chen Bin">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="cilibili">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/24/SPARK学习笔记/sparkStreaming笔记/" itemprop="url">SparkStreaming</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-07-24T11:31:41+08:00">
                2019-07-24
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="SparkStreaming"><a href="#SparkStreaming" class="headerlink" title="SparkStreaming"></a>SparkStreaming</h1><h2 id="SparkStreaming-1"><a href="#SparkStreaming-1" class="headerlink" title="SparkStreaming"></a>SparkStreaming</h2><p>导入spark-streaming依赖</p>
<h3 id="整合Kafka"><a href="#整合Kafka" class="headerlink" title="整合Kafka"></a>整合Kafka</h3><p>导入spark-streaming-kafka-0-8_2.11依赖（0-8kafka版本，2.11scala版本，0.8和0.10两个版本）</p>
<p><strong>注意</strong> ：version可能是要求和spark-core版本一致，详见<a href="sparkStreaming笔记/错误日志.xml">错误日志</a></p>
<h4 id="无状态WordCount"><a href="#无状态WordCount" class="headerlink" title="无状态WordCount"></a>无状态WordCount</h4><ol>
<li>创建StreamingContxt（用sparkConf）</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"KafkaWordCount"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>)) <span class="comment">//内部创建sparkContext(conf)</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li>Kafka参数配置</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> zkQuorum = <span class="string">"node-1:2181,node-2:2181,node-3:2181"</span></span><br><span class="line"><span class="keyword">val</span> groupId = <span class="string">"g1"</span></span><br><span class="line"><span class="keyword">val</span> topic = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>](<span class="string">"xiaoniuabc"</span> -&gt; <span class="number">1</span>) <span class="comment">//1是topic对应的线程数</span></span><br></pre></td></tr></table></figure>

<ol start="3">
<li>AWL方式创建DStream（write ahead logs ）</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Kafak的ReceiverInputDStream[(String, String)]里面装的是一个元组（key，value是实际内容）</span></span><br><span class="line"><span class="keyword">val</span> data: <span class="type">ReceiverInputDStream</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">KafkaUtils</span>.createStream(ssc, zkQuorum, groupId, topic)</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>对DStream进行操作</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//取出value里写的实际内容</span></span><br><span class="line"><span class="keyword">val</span> lines: <span class="type">DStream</span>[<span class="type">String</span>] = data.map(_._2)</span><br><span class="line"><span class="comment">//对DSteam进行操作，你操作这个抽象（代理，描述），就像操作一个本地的集合一样</span></span><br><span class="line"><span class="keyword">val</span> words: <span class="type">DStream</span>[<span class="type">String</span>] = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"><span class="keyword">val</span> wordAndOne: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = words.map((_, <span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> reduced: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordAndOne.reduceByKey(_+_)</span><br><span class="line">reduced.print()</span><br></pre></td></tr></table></figure>

<ol start="5">
<li>SparkStreaming程序启动和退出</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//启动sparksteaming程序</span></span><br><span class="line">ssc.start()</span><br><span class="line"><span class="comment">//等待优雅的退出</span></span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></table></figure>

<h4 id="有状态WordCount"><a href="#有状态WordCount" class="headerlink" title="有状态WordCount"></a>有状态WordCount</h4><ol start="0">
<li>可以在main方法外面定义updateStateByKey的函数参数</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 第一个参数：聚合的key，就是单词</span></span><br><span class="line"><span class="comment">  * 第二个参数：当前批次产生批次该单词在每一个分区出现的次数</span></span><br><span class="line"><span class="comment">  * 第三个参数：初始值或累加的中间结果</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="keyword">val</span> updateFunc = (iter: <span class="type">Iterator</span>[(<span class="type">String</span>, <span class="type">Seq</span>[<span class="type">Int</span>], <span class="type">Option</span>[<span class="type">Int</span>])]) =&gt; &#123;</span><br><span class="line">  iter.map(t =&gt; (t._1, t._2.sum + t._3.getOrElse(<span class="number">0</span>)))    <span class="comment">// -----&gt;方式一</span></span><br><span class="line">  <span class="comment">//iter.map&#123; case(x, y, z) =&gt; (x, y.sum + z.getOrElse(0))&#125; -----&gt;方式二（模式匹配）</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol>
<li><p>创建StreamingContxt（用sparkConf）</p>
</li>
<li><p><strong>设置checkpoint</strong></p>
</li>
</ol>
<p>​    <strong>注意</strong> ：设置了checkpoint过后，对DStream的操作中间结果都涉及到写入写出，<u>所以对象可能会存在序列化问题</u> （比如DStream的foreachRDD方法里面代码在Driver端执行，但用到外部对象也会出现序列化问题）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//如果要使用课更新历史数据（累加），那么就要把终结结果保存起来</span></span><br><span class="line">ssc.checkpoint(<span class="string">"./ck"</span>)</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>Kafka参数配置</li>
<li>AWL方式创建DStream（write ahead logs ）</li>
<li>对DStream进行操作</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//单词和一组合在一起</span></span><br><span class="line"><span class="keyword">val</span> wordAndOne: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = words.map((_, <span class="number">1</span>))</span><br><span class="line"><span class="comment">//聚合所有批次数据（但是只记录程序开始执行之后的数据，程序停止则丢失数据）</span></span><br><span class="line"><span class="comment">//ssc.sparkContext.defaultParallelism表示分区器使用默认的分区数量</span></span><br><span class="line"><span class="keyword">val</span> reduced: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordAndOne.updateStateByKey(updateFunc, <span class="keyword">new</span> <span class="type">HashPartitioner</span>(ssc.sparkContext.defaultParallelism), <span class="literal">true</span>) </span><br><span class="line"><span class="comment">//打印结果(Action)</span></span><br><span class="line">reduced.print()</span><br></pre></td></tr></table></figure>

<ol start="6">
<li>SparkStreaming程序启动和退出</li>
</ol>
<h3 id="直连方式-SparkStreaming-kafka-0-8"><a href="#直连方式-SparkStreaming-kafka-0-8" class="headerlink" title="直连方式-SparkStreaming-kafka-0.8"></a>直连方式-SparkStreaming-kafka-0.8</h3><h4 id="1-创建StreamingContxt（用sparkConf）"><a href="#1-创建StreamingContxt（用sparkConf）" class="headerlink" title="1. 创建StreamingContxt（用sparkConf）"></a>1. 创建StreamingContxt（用sparkConf）</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建SparkConf</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"OrderCount"</span>).setMaster(<span class="string">"local[4]"</span>)</span><br><span class="line"><span class="comment">//创建SparkStreaming，并设置间隔时间</span></span><br><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Duration</span>(<span class="number">5000</span>))</span><br></pre></td></tr></table></figure>

<h4 id="2-Kafka参数配置"><a href="#2-Kafka参数配置" class="headerlink" title="2. Kafka参数配置"></a>2. Kafka参数配置</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//指定组名</span></span><br><span class="line"><span class="keyword">val</span> group = <span class="string">"g001"</span></span><br><span class="line"><span class="comment">//指定消费的 topic 名字</span></span><br><span class="line"><span class="keyword">val</span> topic = <span class="string">"wordcount"</span></span><br><span class="line"><span class="comment">//指定kafka的broker地址(sparkStream的Task直连到kafka的分区上，用更加底层的API消费，效率更高)</span></span><br><span class="line"><span class="keyword">val</span> brokerList = <span class="string">"node-4:9092,node-5:9092,node-6:9092"</span></span><br><span class="line"><span class="comment">//指定zk的地址，后期更新消费的偏移量时使用(以后可以使用Redis、MySQL来记录偏移量)</span></span><br><span class="line"><span class="keyword">val</span> zkQuorum = <span class="string">"node-1:2181,node-2:2181,node-3:2181"</span></span><br><span class="line"><span class="comment">//创建 stream 时使用的 topic 名字集合，SparkStreaming可同时消费多个topic</span></span><br><span class="line"><span class="keyword">val</span> topics: <span class="type">Set</span>[<span class="type">String</span>] = <span class="type">Set</span>(topic)</span><br><span class="line"></span><br><span class="line"><span class="comment">//准备kafka的参数</span></span><br><span class="line"><span class="keyword">val</span> kafkaParams = <span class="type">Map</span>(</span><br><span class="line">    <span class="comment">//"key.deserializer" -&gt; classOf[StringDeserializer],</span></span><br><span class="line">    <span class="comment">//"value.deserializer" -&gt; classOf[StringDeserializer],</span></span><br><span class="line">    <span class="comment">//"deserializer.encoding" -&gt; "GB2312", //配置读取Kafka中数据的编码</span></span><br><span class="line">    <span class="string">"metadata.broker.list"</span> -&gt; brokerList,</span><br><span class="line">    <span class="string">"group.id"</span> -&gt; group,</span><br><span class="line">    <span class="comment">//从头（上次偏移量）开始读取数据，LargestTimeString:启动后开始读数据</span></span><br><span class="line">    <span class="string">"auto.offset.reset"</span> -&gt; kafka.api.<span class="type">OffsetRequest</span>.<span class="type">SmallestTimeString</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h4 id="3-创建zookeeper偏移量保存路径"><a href="#3-创建zookeeper偏移量保存路径" class="headerlink" title="3. 创建zookeeper偏移量保存路径"></a>3. 创建zookeeper偏移量保存路径</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建一个 ZKGroupTopicDirs 对象,其实是指定往zk中写入数据的目录，用于保存偏移量</span></span><br><span class="line"><span class="keyword">val</span> topicDirs = <span class="keyword">new</span> <span class="type">ZKGroupTopicDirs</span>(group, topic)</span><br><span class="line"><span class="comment">//获取 zookeeper 中的路径 "/g001/offsets/wordcount/"</span></span><br><span class="line"><span class="keyword">val</span> zkTopicPath = <span class="string">s"<span class="subst">$&#123;topicDirs.consumerOffsetDir&#125;</span>"</span></span><br></pre></td></tr></table></figure>

<h4 id="4-创建zookeeper客户端"><a href="#4-创建zookeeper客户端" class="headerlink" title="4. 创建zookeeper客户端"></a>4. 创建zookeeper客户端</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//zookeeper 的host 和 ip，创建一个 client,用于跟新偏移量量的</span></span><br><span class="line"><span class="comment">//是zookeeper的客户端，可以从zk中读取偏移量数据，并更新偏移量</span></span><br><span class="line"><span class="keyword">val</span> zkClient = <span class="keyword">new</span> <span class="type">ZkClient</span>(zkQuorum)</span><br></pre></td></tr></table></figure>

<h4 id="5-定义保存kafkaSteam和偏移量的变量"><a href="#5-定义保存kafkaSteam和偏移量的变量" class="headerlink" title="5. 定义保存kafkaSteam和偏移量的变量"></a>5. 定义保存<strong>kafkaSteam</strong>和<strong>偏移量</strong>的变量</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//kafkaStream的值根据是否记录过便宜量分两种方法读取</span></span><br><span class="line"><span class="keyword">var</span> kafkaStream: <span class="type">InputDStream</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="literal">null</span></span><br><span class="line"><span class="comment">//如果 zookeeper 中有保存 offset，我们会利用这个 offset 作为 kafkaStream 的起始位置</span></span><br><span class="line"><span class="keyword">var</span> fromOffsets: <span class="type">Map</span>[<span class="type">TopicAndPartition</span>, <span class="type">Long</span>] = <span class="type">Map</span>()</span><br></pre></td></tr></table></figure>

<h4 id="6-查询同个group-amp-topic下分区数量，分是否保存过offset分别处理"><a href="#6-查询同个group-amp-topic下分区数量，分是否保存过offset分别处理" class="headerlink" title="6. 查询同个group&amp;topic下分区数量，分是否保存过offset分别处理"></a>6. 查询同个group&amp;topic下分区数量，分是否保存过offset分别处理</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//查询该路径下是否子节点（默认有字节点为我们自己保存不同 partition 时生成的）</span></span><br><span class="line"><span class="comment">// /g001/offsets/wordcount/0/10001"  --&gt; 0号分区/偏移量10001</span></span><br><span class="line"><span class="comment">// /g001/offsets/wordcount/1/30001"  ...</span></span><br><span class="line"><span class="comment">// /g001/offsets/wordcount/2/10001"  ...</span></span><br><span class="line"><span class="comment">//zkTopicPath  -&gt; /g001/offsets/wordcount/</span></span><br><span class="line"><span class="keyword">val</span> children = zkClient.countChildren(zkTopicPath)<span class="comment">//返回分区数量</span></span><br></pre></td></tr></table></figure>

<h4 id="7-调用KafkaUtils-createDirectStream创建kafkaStream"><a href="#7-调用KafkaUtils-createDirectStream创建kafkaStream" class="headerlink" title="7. 调用KafkaUtils.createDirectStream创建kafkaStream"></a>7. 调用KafkaUtils.createDirectStream创建kafkaStream</h4><ul>
<li><p>如存在偏移量</p>
<ol>
<li>取出偏移量存入之前创建的保存偏移量的参数(fromOffsets)中</li>
<li>创建函数将MessageAndMetadata[String, String]取出message转成tupple类型</li>
<li>调用createDirectStream，传入ssc、kafka参数、fromOffsets、和MessageAndMetadata函数</li>
</ol>
</li>
<li><p>未存在偏移量</p>
<ol>
<li><p>直接调用createDirectStream，传入ssc、kafka参数、和topics  (Set[String]类型) </p>
<p>注：可能因为fromOffsets: Map[TopicAndPartition, Long]存在topic信息所以不用传topics了</p>
</li>
</ol>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//如果保存过 offset</span></span><br><span class="line"><span class="keyword">if</span> (children &gt; <span class="number">0</span>) &#123;</span><br><span class="line">  <span class="keyword">for</span> (i &lt;- <span class="number">0</span> until children) &#123;</span><br><span class="line">    <span class="comment">// 假如存在偏移量保存路径：/g001/offsets/wordcount/0/10001</span></span><br><span class="line">    <span class="comment">// $zkTopicPath/$&#123;i&#125;:g001/offsets/wordcount/0</span></span><br><span class="line">	<span class="comment">// partitionOffset:10001</span></span><br><span class="line">    <span class="keyword">val</span> partitionOffset = zkClient.readData[<span class="type">String</span>](<span class="string">s"<span class="subst">$zkTopicPath</span>/<span class="subst">$&#123;i&#125;</span>"</span>)</span><br><span class="line">    <span class="comment">// wordcount/0</span></span><br><span class="line">    <span class="keyword">val</span> tp = <span class="type">TopicAndPartition</span>(topic, i)</span><br><span class="line">    <span class="comment">//将不同 partition 对应的 offset 增加到 fromOffsets 中</span></span><br><span class="line">    <span class="comment">// wordcount/0 -&gt; 10001</span></span><br><span class="line">    fromOffsets += (tp -&gt; partitionOffset.toLong)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//Key: kafka的key   values: "hello tom hello jerry"</span></span><br><span class="line">  <span class="comment">//这个会将 kafka 的消息进行 transform，最终 kafak 的数据都会变成 (kafka的key, message) 这样的 tuple，也可以用（mmd.topic(), mmd.message()）获得topic,value这样的数据</span></span><br><span class="line">  <span class="keyword">val</span> messageHandler = (mmd: <span class="type">MessageAndMetadata</span>[<span class="type">String</span>, <span class="type">String</span>]) =&gt; (mmd.key(), mmd.message())																</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//通过KafkaUtils创建直连的DStream（fromOffsets参数的作用是:按照前面计算好了的偏移量继续消费数据）</span></span><br><span class="line">  <span class="comment">//[String, String, StringDecoder, StringDecoder,     (String, String)]</span></span><br><span class="line">  <span class="comment">//  key    value    key的解码方式   value的解码方式 </span></span><br><span class="line">  kafkaStream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>, <span class="type">StringDecoder</span>, <span class="type">StringDecoder</span>, (<span class="type">String</span>, <span class="type">String</span>)](ssc, kafkaParams, fromOffsets, messageHandler)</span><br><span class="line">&#125; </span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="comment">//如果未保存，根据 kafkaParam 的配置使用最新(largest)或者最旧的（smallest） offset</span></span><br><span class="line">  kafkaStream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>, <span class="type">StringDecoder</span>, <span class="type">StringDecoder</span>](ssc, kafkaParams, topics)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="8-定义保存偏移量范围的变量"><a href="#8-定义保存偏移量范围的变量" class="headerlink" title="8. 定义保存偏移量范围的变量"></a>8. 定义保存<strong>偏移量范围</strong>的变量</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//偏移量的范围</span></span><br><span class="line"><span class="keyword">var</span> offsetRanges = <span class="type">Array</span>[<span class="type">OffsetRange</span>]()</span><br></pre></td></tr></table></figure>

<h4 id="9-调用DStream的transform方法将kafkaStream强转成HasOffsetRanges，获取偏移量范围信息并保存"><a href="#9-调用DStream的transform方法将kafkaStream强转成HasOffsetRanges，获取偏移量范围信息并保存" class="headerlink" title="9. 调用DStream的transform方法将kafkaStream强转成HasOffsetRanges，获取偏移量范围信息并保存"></a>9. 调用DStream的transform方法将kafkaStream强转成HasOffsetRanges，获取偏移量范围信息并保存</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//从kafka读取的消息，DStream的Transform方法可以将当前批次的RDD获取出来</span></span><br><span class="line"><span class="comment">//该transform方法计算获取到当前批次RDD,然后将RDD的偏移量取出来，然后在将RDD返回到DStream</span></span><br><span class="line"><span class="keyword">val</span> transform: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">String</span>)] = kafkaStream.transform &#123; rdd =&gt;</span><br><span class="line">  <span class="comment">//得到该 rdd 对应 kafka 的消息的 offset</span></span><br><span class="line">  <span class="comment">//该RDD是一个KafkaRDD，可以获得偏移量的范围</span></span><br><span class="line">  offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</span><br><span class="line">  rdd<span class="comment">//将原kafkaStream原封返回</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="10-只获取kafkaStream中的message"><a href="#10-只获取kafkaStream中的message" class="headerlink" title="10. 只获取kafkaStream中的message"></a>10. 只获取kafkaStream中的message</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> messages: <span class="type">DStream</span>[<span class="type">String</span>] = transform.map(_._2)</span><br></pre></td></tr></table></figure>

<h4 id="11-对DStream调用foreachRDD进行操作每个RDD流"><a href="#11-对DStream调用foreachRDD进行操作每个RDD流" class="headerlink" title="11. 对DStream调用foreachRDD进行操作每个RDD流"></a>11. 对DStream调用foreachRDD进行操作每个RDD流</h4><ul>
<li><h4 id="将偏移量保存到zookeeper的gropu-offset-topic-partition路径下"><a href="#将偏移量保存到zookeeper的gropu-offset-topic-partition路径下" class="headerlink" title="将偏移量保存到zookeeper的gropu/offset/topic/partition路径下"></a>将偏移量保存到zookeeper的gropu/offset/topic/partition路径下</h4></li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//依次迭代DStream中的RDD</span></span><br><span class="line"><span class="comment">//注意!：kafkaStream.foreachRDD里面的业务逻辑是在Driver端执行、kafkaRDD.maps里定义的方法才是在Excutor里执行，maps方法是在Driver端执行</span></span><br><span class="line"><span class="comment">//foreachRDD只是把每个RDD拿出来，没有触发Action</span></span><br><span class="line">messages.foreachRDD &#123; rdd =&gt;</span><br><span class="line">  <span class="comment">//对RDD进行操作，触发Action</span></span><br><span class="line">  rdd.foreachPartition(partition =&gt;</span><br><span class="line">    partition.foreach(x =&gt; &#123;</span><br><span class="line">      println(x)</span><br><span class="line">    &#125;)</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (o &lt;- offsetRanges) &#123;</span><br><span class="line">    <span class="comment">//  /g001/offsets/wordcount/0</span></span><br><span class="line">    <span class="keyword">val</span> zkPath = <span class="string">s"<span class="subst">$&#123;topicDirs.consumerOffsetDir&#125;</span>/<span class="subst">$&#123;o.partition&#125;</span>"</span></span><br><span class="line">    <span class="comment">//将该 partition 的 offset 保存到 zookeeper</span></span><br><span class="line">    <span class="comment">//  /g001/offsets/wordcount/0/20000（untilOffset是截至的偏移量）</span></span><br><span class="line">    <span class="type">ZkUtils</span>.updatePersistentPath(zkClient, zkPath, o.untilOffset.toString)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="12-SparkStreaming程序启动和退出"><a href="#12-SparkStreaming程序启动和退出" class="headerlink" title="12. SparkStreaming程序启动和退出"></a>12. SparkStreaming程序启动和退出</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssc.start()</span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></table></figure>

<p><strong>备注</strong>：也可以在kafkaStream.foreachRDD中对kafkaRDD强转获取到偏移量（只能对kafkaRDD进行强转）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//直连方式只有在KafkaDStream的RDD中才能获取偏移量，那么就不能再调用DStream的Transformation</span></span><br><span class="line"><span class="comment">//所以只能在kafkaStream调用foreachRDD，获取RDD的偏移量，然后就是对RDD进行操作了</span></span><br><span class="line"><span class="comment">//依次迭代KafkaDStream中的KafkaRDD</span></span><br><span class="line">kafkaStream.foreachRDD &#123; kafkaRDD =&gt;</span><br><span class="line">  <span class="comment">//只有KafkaRDD可以强转成HasOffsetRanges，并获取到偏移量</span></span><br><span class="line">  offsetRanges = kafkaRDD.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</span><br><span class="line">  <span class="keyword">val</span> lines: <span class="type">RDD</span>[<span class="type">String</span>] = kafkaRDD.map(_._2)</span><br></pre></td></tr></table></figure>

<h4 id="13-补充：kafka直连方式消费多个topic"><a href="#13-补充：kafka直连方式消费多个topic" class="headerlink" title="13. 补充：kafka直连方式消费多个topic"></a>13. 补充：<a href="https://blog.csdn.net/Lu_Xiao_Yue/article/details/84075565" target="_blank" rel="noopener">kafka直连方式消费多个topic</a></h4><h4 id="Kafka知识补充"><a href="#Kafka知识补充" class="headerlink" title="Kafka知识补充"></a>Kafka知识补充</h4><h5 id="1-Kafka分区"><a href="#1-Kafka分区" class="headerlink" title="1. Kafka分区"></a>1. Kafka分区</h5><ul>
<li>0、1、2代表数据的不同分区</li>
<li>0`、1`、2`代表不同的副本</li>
<li>生产者向一个leader分区发送数据，消费者也向leader分区消费数据（可以配置消费者向非leader分区消费）</li>
<li>数据的分区数量可以任意，副本数量不大于启动kafka机器的数量<br><img src="sparkStreaming%E7%AC%94%E8%AE%B0%5Ckafka%E5%88%86%E5%8C%BA%E5%8E%9F%E7%90%86.png" alt="Kafka分区原理"></li>
</ul>
<h5 id="2-kafka直连"><a href="#2-kafka直连" class="headerlink" title="2. kafka直连"></a>2. kafka直连</h5><p>Direct方式采用Kafka简单的consumer api方式来读取数据，无需经由ZooKeeper，此种方式不再需要专门Receiver来持续不断读取数据。当batch任务触发时，由Executor读取数据，并参与到其他Executor的数据计算过程中去。driver来决定读取多少offsets，并将offsets交由checkpoints来维护。将触发下次batch任务，再由Executor读取Kafka数据并计算。从此过程我们可以发现Direct方式无需Receiver读取数据，而是需要计算时再读取数据，所以Direct方式的数据消费对内存的要求不高，只需要考虑批量计算所需要的内存即可；另外batch任务堆积时，也不会影响数据堆积。其具体读取方式如下图：</p>
<p>Receiver方式：<img src="sparkStreaming%E7%AC%94%E8%AE%B0%5CReceiver%E6%96%B9%E5%BC%8F.png" alt="Receiver方式"></p>
<p>直连方式：<img src="sparkStreaming%E7%AC%94%E8%AE%B0%5C%E7%9B%B4%E8%BF%9E%E6%96%B9%E5%BC%8F.png" alt></p>
<h3 id="直连方式-SparkStreaming-kafka-0-10"><a href="#直连方式-SparkStreaming-kafka-0-10" class="headerlink" title="直连方式-SparkStreaming-kafka-0.10"></a>直连方式-SparkStreaming-kafka-0.10</h3><h4 id="1-创建StreamingContxt（用sparkConf）-1"><a href="#1-创建StreamingContxt（用sparkConf）-1" class="headerlink" title="1. 创建StreamingContxt（用sparkConf）"></a>1. 创建StreamingContxt（用sparkConf）</h4><h4 id="2-Kafka参数配置-1"><a href="#2-Kafka参数配置-1" class="headerlink" title="2. Kafka参数配置"></a>2. Kafka参数配置</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> topics = <span class="type">Array</span>(topic)	<span class="comment">//数组方式保存topics</span></span><br><span class="line"><span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">    <span class="comment">//即0.8版的broker-list</span></span><br><span class="line">   <span class="string">"bootstrap.servers"</span> -&gt; <span class="string">"node-1:9092,node-2:9092,node-3:9092"</span>,</span><br><span class="line">   <span class="string">"key.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">   <span class="string">"value.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">   <span class="string">"group.id"</span> -&gt; group,</span><br><span class="line">   <span class="string">"auto.offset.reset"</span> -&gt; <span class="string">"earliest"</span>, <span class="comment">// lastest</span></span><br><span class="line">   <span class="string">"enable.auto.commit"</span> -&gt; (<span class="literal">false</span>: java.lang.<span class="type">Boolean</span>) <span class="comment">// false代表自己写程序提交偏移量 </span></span><br><span class="line"> )</span><br></pre></td></tr></table></figure>

<h4 id="3-创建SparkDStream-一个API"><a href="#3-创建SparkDStream-一个API" class="headerlink" title="3. 创建SparkDStream(一个API)"></a>3. 创建SparkDStream(一个API)</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//用直连方式读取kafka中的数据，在Kafka中记录读取偏移量</span></span><br><span class="line"><span class="comment">//spark-kafka-0.8把偏移量保存在zookeeper里，也可存在redis等里</span></span><br><span class="line"><span class="keyword">val</span> stream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">  streamingContext,</span><br><span class="line">  <span class="comment">//位置策略（如果kafka和spark程序部署在一起，会有最优位置）</span></span><br><span class="line">  <span class="type">PreferConsistent</span>,</span><br><span class="line">  <span class="comment">//订阅的策略（可以指定用正则的方式读取topic，比如my-ordsers-.*）</span></span><br><span class="line">  <span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topics, kafkaParams)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h4 id="4-获取跟新偏移量，操作DStream"><a href="#4-获取跟新偏移量，操作DStream" class="headerlink" title="4. 获取跟新偏移量，操作DStream"></a>4. 获取跟新偏移量，操作DStream</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//迭代DStream中的RDD，将每一个时间点对于的RDD拿出来</span></span><br><span class="line">stream.foreachRDD &#123; rdd =&gt;</span><br><span class="line">  <span class="keyword">if</span>(!rdd.isEmpty()) &#123;</span><br><span class="line">    <span class="comment">//获取该RDD对于的偏移量</span></span><br><span class="line">    <span class="keyword">val</span> offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</span><br><span class="line">	<span class="comment">//拿出对于的数据，foreach是一个aciton</span></span><br><span class="line">    rdd.foreach&#123; line =&gt;</span><br><span class="line">		<span class="comment">//Kafka在消费偏移量之后的数据不一定按顺序进行消费（因为有多个Consumer同时进行消费）</span></span><br><span class="line">		println(line.key() + <span class="string">" "</span> + line.value())</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//更新偏移量</span></span><br><span class="line">    <span class="comment">// some time later, after outputs have completed</span></span><br><span class="line">    stream.asInstanceOf[<span class="type">CanCommitOffsets</span>].commitAsync(offsetRanges)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h4 id="5-SparkStreaming程序启动和退出"><a href="#5-SparkStreaming程序启动和退出" class="headerlink" title="5. SparkStreaming程序启动和退出"></a>5. SparkStreaming程序启动和退出</h4><h3 id="订单项目示例（离线和实时）-非常重要"><a href="#订单项目示例（离线和实时）-非常重要" class="headerlink" title="订单项目示例（离线和实时）*非常重要*"></a>订单项目示例（离线和实时）*非常重要*</h3><p>数据：A 202.106.196.115 手机 iPhone8 8000</p>
<h4 id="0-用到的工具类（Object）："><a href="#0-用到的工具类（Object）：" class="headerlink" title="0. 用到的工具类（Object）："></a>0. 用到的工具类（Object）：</h4><ul>
<li>离线计算 IP-地址 规则</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">broadcastIpRules</span></span>(ssc: <span class="type">StreamingContext</span>, ipRulesPath: <span class="type">String</span>): <span class="type">Broadcast</span>[<span class="type">Array</span>[(<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">String</span>)]] = &#123;</span><br><span class="line">  <span class="comment">//现获取sparkContext</span></span><br><span class="line">  <span class="keyword">val</span> sc = ssc.sparkContext</span><br><span class="line">  <span class="keyword">val</span> rulesLines:<span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(ipRulesPath)</span><br><span class="line">  <span class="comment">//整理ip规则数据</span></span><br><span class="line">  <span class="keyword">val</span> ipRulesRDD: <span class="type">RDD</span>[(<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">String</span>)] = rulesLines.map(line =&gt; &#123;</span><br><span class="line"> 	...</span><br><span class="line">    (startNum, endNum, province)</span><br><span class="line">  &#125;)</span><br><span class="line">  <span class="comment">//将分散在多个Executor中的部分IP规则收集到Driver端</span></span><br><span class="line">  <span class="keyword">val</span> rulesInDriver: <span class="type">Array</span>[(<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">String</span>)] = ipRulesRDD.collect()</span><br><span class="line">  <span class="comment">//将Driver端的数据广播到Executor</span></span><br><span class="line">  <span class="comment">//广播变量的引用（还在Driver端）</span></span><br><span class="line">  sc.broadcast(rulesInDriver)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>计算成交总金额（一个批次收入存入Redis，流数据不断累加）</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculateIncome</span></span>(fields: <span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">String</span>]]) = &#123;</span><br><span class="line">  <span class="comment">//将数据计算后写入到Reids</span></span><br><span class="line">  <span class="keyword">val</span> priceRDD: <span class="type">RDD</span>[<span class="type">Double</span>] = fields.map(arr =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> price = arr(<span class="number">4</span>).toDouble</span><br><span class="line">    price</span><br><span class="line">  &#125;)</span><br><span class="line">  <span class="comment">//reduce是一个Action，会把结果返回到Driver端</span></span><br><span class="line">  <span class="comment">//将当前批次的总金额返回了</span></span><br><span class="line">  <span class="keyword">val</span> sum: <span class="type">Double</span> = priceRDD.reduce(_+_)</span><br><span class="line">  <span class="comment">//获取一个jedis连接（在Driver端创建）</span></span><br><span class="line">  <span class="keyword">val</span> conn = <span class="type">JedisConnectionPool</span>.getConnection()</span><br><span class="line">  <span class="comment">//将历史值和当前的值进行累加</span></span><br><span class="line">  <span class="comment">//conn.set(Constant.TOTAL_INCOME, sum.toString)</span></span><br><span class="line">  conn.incrByFloat(<span class="type">Constant</span>.<span class="type">TOTAL_INCOME</span>, sum)</span><br><span class="line">  <span class="comment">//释放连接</span></span><br><span class="line">  conn.close()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>计算商品分类成交总金额（一个批次收入存入Redis，流数据不断累加）</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculateItem</span></span>(fields: <span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">String</span>]]) = &#123;</span><br><span class="line">  <span class="comment">//备注：对field的map方法是在哪一端调用的呢？Driver，只是map里面的方法还没执行</span></span><br><span class="line">  <span class="keyword">val</span> itemAndPrice: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Double</span>)] = fields.map(arr =&gt; &#123;</span><br><span class="line">    <span class="comment">//分类</span></span><br><span class="line">    <span class="keyword">val</span> item = arr(<span class="number">2</span>)</span><br><span class="line">    <span class="comment">//金额</span></span><br><span class="line">    <span class="keyword">val</span> parice = arr(<span class="number">4</span>).toDouble</span><br><span class="line">    (item, parice)</span><br><span class="line">  &#125;)</span><br><span class="line">  <span class="comment">//按照商品分类进行聚合</span></span><br><span class="line">  <span class="keyword">val</span> reduced: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Double</span>)] = itemAndPrice.reduceByKey(_+_)</span><br><span class="line">  <span class="comment">//将当前批次的数据累加到Redis中</span></span><br><span class="line">  <span class="comment">//foreachPartition是一个Action</span></span><br><span class="line">  <span class="comment">//现在这种方式，jeids的连接是在哪一端创建的（Driver）</span></span><br><span class="line">  <span class="comment">//在Driver端拿Jedis连接不好(要序列化)</span></span><br><span class="line">  <span class="comment">//val conn = JedisConnectionPool.getConnection()</span></span><br><span class="line">  reduced.foreachPartition(part =&gt; &#123;</span><br><span class="line">    <span class="comment">//获取一个Jedis连接</span></span><br><span class="line">    <span class="comment">//这个连接其实是在Executor中的获取的</span></span><br><span class="line">    <span class="comment">//JedisConnectionPool在一个Executor进程中只有1个实例（单例）</span></span><br><span class="line">    <span class="keyword">val</span> conn = <span class="type">JedisConnectionPool</span>.getConnection()</span><br><span class="line">    part.foreach(t =&gt; &#123;</span><br><span class="line">      <span class="comment">//一个连接更新多条数据</span></span><br><span class="line">      conn.incrByFloat(t._1, t._2)</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment">//将当前分区中的数据跟新完在关闭连接</span></span><br><span class="line">    conn.close()</span><br><span class="line">  &#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>计算省份成交总金额（一个批次收入存入Redis，流数据不断累加）</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculateZone</span></span>(fields: <span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">String</span>]], broadcastRef: <span class="type">Broadcast</span>[<span class="type">Array</span>[(<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">String</span>)]]) = &#123;</span><br><span class="line">  <span class="keyword">val</span> provinceAndPrice: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Double</span>)] = fields.map(arr =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> ip = arr(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> price = arr(<span class="number">4</span>).toDouble</span><br><span class="line">    <span class="keyword">val</span> ipNum = <span class="type">MyUtils</span>.ip2Long(ip)</span><br><span class="line">    <span class="comment">//在Executor中获取到广播的全部规则</span></span><br><span class="line">    <span class="keyword">val</span> allRules: <span class="type">Array</span>[(<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">String</span>)] = broadcastRef.value</span><br><span class="line">    <span class="comment">//二分法查找,根据Ip计算归属地</span></span><br><span class="line">    <span class="keyword">val</span> index = <span class="type">MyUtils</span>.binarySearch(allRules, ipNum)</span><br><span class="line">    <span class="keyword">var</span> province = <span class="string">"未知"</span></span><br><span class="line">    <span class="keyword">if</span> (index != <span class="number">-1</span>) &#123;</span><br><span class="line">      province = allRules(index)._3</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//省份，订单金额</span></span><br><span class="line">    (province, price)</span><br><span class="line">  &#125;)</span><br><span class="line">  <span class="comment">//按省份进行聚合</span></span><br><span class="line">  <span class="keyword">val</span> reduced: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Double</span>)] = provinceAndPrice.reduceByKey(_+_)</span><br><span class="line">  <span class="comment">//将数据跟新到Redis</span></span><br><span class="line"><span class="comment">//key不多的话可以用.foreach</span></span><br><span class="line">  reduced.foreachPartition(part =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> conn = <span class="type">JedisConnectionPool</span>.getConnection()</span><br><span class="line">    part.foreach(t =&gt; &#123;</span><br><span class="line">      conn.incrByFloat(t._1, t._2)</span><br><span class="line">    &#125;)</span><br><span class="line">    conn.close()</span><br><span class="line">  &#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="1-创建StreamingContxt（用sparkConf）-2"><a href="#1-创建StreamingContxt（用sparkConf）-2" class="headerlink" title="1. 创建StreamingContxt（用sparkConf）"></a>1. 创建StreamingContxt（用sparkConf）</h4><h4 id="2-广播离线计算的-IP-地址-规则"><a href="#2-广播离线计算的-IP-地址-规则" class="headerlink" title="*2. *广播离线计算的 IP-地址 规则**"></a>*<em>2. *</em>广播离线计算的 IP-地址 规则**</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> broadcastRef: <span class="type">Broadcast</span>[<span class="type">Array</span>[(<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">String</span>)]] = <span class="type">IPUtils</span>.broadcastIpRules(ssc, <span class="string">"/Users/zx/Desktop/temp/spark-24/spark-4/ip/ip.txt"</span>)</span><br></pre></td></tr></table></figure>

<h4 id="3-Kafka参数配置"><a href="#3-Kafka参数配置" class="headerlink" title="3. Kafka参数配置"></a>3. Kafka参数配置</h4><h4 id="4-创建zookeeper偏移量保存路径"><a href="#4-创建zookeeper偏移量保存路径" class="headerlink" title="4. 创建zookeeper偏移量保存路径"></a>4. 创建zookeeper偏移量保存路径</h4><h4 id="5-创建zookeeper客户端"><a href="#5-创建zookeeper客户端" class="headerlink" title="5. 创建zookeeper客户端"></a>5. 创建zookeeper客户端</h4><h4 id="6-定义保存kafkaSteam和偏移量的变量"><a href="#6-定义保存kafkaSteam和偏移量的变量" class="headerlink" title="6. 定义保存kafkaSteam和偏移量的变量"></a>6. 定义保存<strong>kafkaSteam</strong>和<strong>偏移量</strong>的变量</h4><h4 id="7-查询同个group-amp-topic下分区数量，分是否保存过offset分别处理"><a href="#7-查询同个group-amp-topic下分区数量，分是否保存过offset分别处理" class="headerlink" title="7. 查询同个group&amp;topic下分区数量，分是否保存过offset分别处理"></a>7. 查询同个group&amp;topic下分区数量，分是否保存过offset分别处理</h4><h4 id="8-调用KafkaUtils-createDirectStream创建kafkaStream"><a href="#8-调用KafkaUtils-createDirectStream创建kafkaStream" class="headerlink" title="8. 调用KafkaUtils.createDirectStream创建kafkaStream"></a>8. 调用KafkaUtils.createDirectStream创建kafkaStream</h4><h4 id="9-定义保存偏移量范围的变量"><a href="#9-定义保存偏移量范围的变量" class="headerlink" title="9. 定义保存偏移量范围的变量"></a>9. 定义保存<strong>偏移量范围</strong>的变量</h4><h4 id="10-调用DStream的transform方法将kafkaStream强转成HasOffsetRanges，获取偏移量范围信息并保存"><a href="#10-调用DStream的transform方法将kafkaStream强转成HasOffsetRanges，获取偏移量范围信息并保存" class="headerlink" title="10. 调用DStream的transform方法将kafkaStream强转成HasOffsetRanges，获取偏移量范围信息并保存"></a>10. 调用DStream的transform方法将kafkaStream强转成HasOffsetRanges，获取偏移量范围信息并保存</h4><h4 id="11-只获取kafkaStream中的message"><a href="#11-只获取kafkaStream中的message" class="headerlink" title="11. 只获取kafkaStream中的message"></a>11. 只获取kafkaStream中的message</h4><h4 id="12-对DStream调用foreachRDD进行操作每个RDD流"><a href="#12-对DStream调用foreachRDD进行操作每个RDD流" class="headerlink" title="***12. 对DStream调用foreachRDD进行操作每个RDD流"></a>***12. 对DStream调用foreachRDD进行操作每个RDD流</h4><ul>
<li><h4 id="将偏移量保存到zookeeper的gropu-offset-topic-partition路径下-1"><a href="#将偏移量保存到zookeeper的gropu-offset-topic-partition路径下-1" class="headerlink" title="将偏移量保存到zookeeper的gropu/offset/topic/partition路径下"></a>将偏移量保存到zookeeper的gropu/offset/topic/partition路径下</h4></li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//注意!：kafkaStream.foreachRDD里面的业务逻辑是在Driver端执行、kafkaRDD.maps是在Excutor里执行</span></span><br><span class="line">kafkaStream.foreachRDD &#123; kafkaRDD =&gt;</span><br><span class="line">  <span class="comment">//判断当前的kafkaStream中的RDD是否有数据</span></span><br><span class="line">  <span class="comment">//注意！！！！！kafkaRDD为空下面的map方法就不能执行（容易踩坑点）</span></span><br><span class="line">  <span class="keyword">if</span>(!kafkaRDD.isEmpty()) &#123;</span><br><span class="line">    <span class="comment">//只有KafkaRDD可以强转成HasOffsetRanges，并获取到偏移量</span></span><br><span class="line">    <span class="comment">//offsetRanges = kafkaRDD.asInstanceOf[HasOffsetRanges].offsetRanges</span></span><br><span class="line">    <span class="comment">//				备注：=============================&gt;在这里获取偏移量范围取代第10步操作</span></span><br><span class="line">      </span><br><span class="line">    <span class="comment">//val lines: RDD[String] = kafkaRDD.map(_._2)</span></span><br><span class="line">	<span class="comment">//				备注：================&gt;在这里获取kafkaStream中的message取代第11步操作</span></span><br><span class="line">      </span><br><span class="line">    <span class="comment">//整理数据</span></span><br><span class="line">	<span class="comment">//备注：RDD在Driver端生成,但里面函数没执行，可以把RDD引用传递给方法</span></span><br><span class="line">    <span class="keyword">val</span> fields: <span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">String</span>]] = lines.map(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//流式数据在这些定义的方法里不断经过处理，计算结果保存到Redis中</span></span><br><span class="line">    <span class="comment">//计算成交总金额</span></span><br><span class="line">    <span class="type">CalculateUtil</span>.calculateIncome(fields)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//计算商品分类金额</span></span><br><span class="line">    <span class="type">CalculateUtil</span>.calculateItem(fields)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//计算区域成交金额</span></span><br><span class="line">    <span class="type">CalculateUtil</span>.calculateZone(fields, broadcastRef)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//偏移量更新在哪一端（Driver）</span></span><br><span class="line">    <span class="keyword">for</span> (o &lt;- offsetRanges) &#123;</span><br><span class="line">      <span class="comment">//  /g001/offsets/wordcount/0</span></span><br><span class="line">      <span class="keyword">val</span> zkPath = <span class="string">s"<span class="subst">$&#123;topicDirs.consumerOffsetDir&#125;</span>/<span class="subst">$&#123;o.partition&#125;</span>"</span></span><br><span class="line">      <span class="comment">//将该 partition 的 offset 保存到 zookeeper</span></span><br><span class="line">      <span class="comment">//  /g001/offsets/wordcount/0/20000</span></span><br><span class="line">      <span class="type">ZkUtils</span>.updatePersistentPath(zkClient, zkPath, o.untilOffset.toString)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="13-SparkStreaming程序启动和退出"><a href="#13-SparkStreaming程序启动和退出" class="headerlink" title="13. SparkStreaming程序启动和退出"></a>13. SparkStreaming程序启动和退出</h4><h3 id="直接监听Socket"><a href="#直接监听Socket" class="headerlink" title="直接监听Socket"></a>直接监听Socket</h3><p>在Linux上用yum安装nc    yum install -y nc</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//线程数量必须要两个以上</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"SteamingWordCount"</span>).setMaster(<span class="string">"local[2]"</span>) </span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="comment">//StreamingContext是对SparkContext的包装，包了一层就增加了实时的功能</span></span><br><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sc, <span class="type">Milliseconds</span>(<span class="number">5000</span>))</span><br><span class="line"><span class="comment">//有了StreamingContext，就可以创建SparkStreaming的抽象了DSteam</span></span><br><span class="line"><span class="comment">//从一个socket端口中读取数据</span></span><br><span class="line"><span class="keyword">val</span> lines: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">"192.168.1.207"</span>, <span class="number">8888</span>)</span><br></pre></td></tr></table></figure>

<h2 id="kafka"><a href="#kafka" class="headerlink" title="kafka"></a>kafka</h2><p>首先在pom里导入kafka依赖（只使用kafka，不使用spark等）</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>0.8.2.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p><strong>Broker</strong>： 安装Kafka服务的那台集群就是一个broker（broker的id要全局唯一）</p>
<p><strong>Producer</strong> ：消息的生产者，负责将数据写入到broker中（push）</p>
<p><strong>Consumer</strong>：消息的消费者，负责从kafka中读取数据（pull），老版本的消费者需要依赖zk，新版本的不需要</p>
<p><strong>Topic</strong>: 主题，相当于是数据的一个分类，不同topic存放不同的数据</p>
<p><strong>partition</strong>：分区，是一个物理的分区，一个分区就算一个文件，一个topic可以有一到多个分区，每一个分区都有自己的副本</p>
<p><strong>replication</strong>：副本，数据保存多少份</p>
<p><strong>Consumer Group</strong>： 消费者组，一个topic可以有多个消费者同时消费，多个消费者如果在一个消费者组中，那么他们不会重复消费数据</p>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>0.安装zookeeper集群，保证zookeeper集群可以使用<br>1.上传Kafka安装包并解压<br>2.修改配置文件 config/server.properties</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">broker.id=0 #每台机器都不唯一1、2、3...</span><br><span class="line">port=9092	#默认的</span><br><span class="line">host.name=node-4  #broker连接的IP地址或主机名,每台机器不同 node-5、6...</span><br><span class="line">log.dirs=/data/kafka  #kafka保存数据的路径</span><br><span class="line">zookeeper.connect=node-1:2181,node-2:2181,node-3:2181</span><br><span class="line">delete.topic.enable=true  #删除topic（否则为删除的时候只是挂了个标签）</span><br></pre></td></tr></table></figure>

<p>3.将配置好的kafka拷贝到其他机器上<br>4.在其他机器上修改broker.id和host.name（不同）<br>5.启动kafka    （副本数量不能大于启动的kafka节点数）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span>KAFKA_HOME/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties 	#-daemon 显示在后台</span><br></pre></td></tr></table></figure>

<h3 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h3><ul>
<li>启动kafka</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>-daemon 显示在后台</span><br><span class="line"><span class="meta">$</span>KAFKA_HOME/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties</span><br></pre></td></tr></table></figure>

<ul>
<li>停止kafka</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span>KAFKA_HOME/bin/kafka-server-stop.sh</span><br></pre></td></tr></table></figure>

<ul>
<li><p>创建topic</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span>KAFKA_HOME/bin/kafka-topics.sh --create --zookeeper node-1:2181,node-2:2181,node-3:2181 --replication-factor 3 --partitions 3 --topic my-topic</span><br></pre></td></tr></table></figure>
</li>
<li><p>列出所有topic</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span>KAFKA_HOME/bin/kafka-topics.sh --list --zookeeper node-1:2181,node-2:2181,node-3:2181</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看某个topic信息（topic信息&lt;topic名字，分区副本等&gt;保存在zookeeper集群中）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span>KAFKA_HOME/bin/kafka-topics.sh --describe --zookeeper node-1:2181,node-2:2181,node-3:2181 --topic my-topic</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动一个命令行的生产者（生产者往kafka里写不需要知道zookeeper在哪）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span>KAFKA_HOME/bin/kafka-console-producer.sh --broker-list node-1:9092,node-1.xiaoniu.xom:9092,node-3:9092 --topic xiaoniu</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动一个命令行的消费者（消费者需要从zookeeper知道数据位置、数据偏移量等信息）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>--from-beginning:可以消费以前的数据</span><br><span class="line"><span class="meta">$</span>KAFKA_HOME/bin/kafka-console-consumer.sh --zookeeper node-1:2181,node-2:2181,node-3:2181 --topic my-topic --from-beginning</span><br></pre></td></tr></table></figure>
</li>
<li><p>消费者连接到borker的地址</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span>KAFKA_HOME/bin/kafka-console-consumer.sh --bootstrap-server node-1:9092,node-2:9092,node-3:9092 --topic xiaoniu --from-beginning</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="Kafka-Java-API"><a href="#Kafka-Java-API" class="headerlink" title="Kafka-Java-API"></a>Kafka-Java-API</h3><h4 id="生产者"><a href="#生产者" class="headerlink" title="生产者"></a>生产者</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProducerDemo</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">		Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">		props.put(<span class="string">"metadata.broker.list"</span>, <span class="string">"node-4:9092,node-5:9092,node-6:9092"</span>);</span><br><span class="line">		props.put(<span class="string">"serializer.class"</span>, <span class="string">"kafka.serializer.StringEncoder"</span>);</span><br><span class="line">		ProducerConfig config = <span class="keyword">new</span> ProducerConfig(props);</span><br><span class="line">		Producer&lt;String, String&gt; producer = <span class="keyword">new</span> Producer&lt;String, String&gt;(config);</span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1001</span>; i &lt;= <span class="number">1100</span>; i++)</span><br><span class="line">			producer.send(<span class="keyword">new</span> KeyedMessage&lt;String, String&gt;(<span class="string">"xiaoniu"</span>, <span class="string">"xiaoniu-msg"</span> + i));<span class="comment">//&lt;topic,message&gt;</span></span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="消费者"><a href="#消费者" class="headerlink" title="消费者"></a>消费者</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ConsumerDemo</span> </span>&#123;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String topic = <span class="string">"xiaoniu"</span>;</span><br><span class="line">	<span class="comment">//一个消费者必须两个线程以上同时消费</span></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Integer threads = <span class="number">2</span>; </span><br><span class="line">    </span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">		Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">		<span class="comment">//此为老版本，需要指定zookeeper地址</span></span><br><span class="line">		props.put(<span class="string">"zookeeper.connect"</span>, <span class="string">"node-1:2181,node-2:2181,node-3:2181"</span>); </span><br><span class="line">		props.put(<span class="string">"group.id"</span>, <span class="string">"vvvvv"</span>);</span><br><span class="line">		<span class="comment">//smallest重最开始消费,largest重消费者启动后产生的数据才消费</span></span><br><span class="line">		<span class="comment">//等同--from-beginning</span></span><br><span class="line">		props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"smallest"</span>);</span><br><span class="line"></span><br><span class="line">		ConsumerConfig config = <span class="keyword">new</span> ConsumerConfig(props);</span><br><span class="line">		ConsumerConnector consumer =Consumer.createJavaConsumerConnector(config);</span><br><span class="line">		Map&lt;String, Integer&gt; topicCountMap = <span class="keyword">new</span> HashMap&lt;String, Integer&gt;();</span><br><span class="line">		topicCountMap.put(topic, threads);</span><br><span class="line">		Map&lt;String, List&lt;KafkaStream&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt;&gt;&gt; consumerMap = consumer.createMessageStreams(topicCountMap);</span><br><span class="line">		List&lt;KafkaStream&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt;&gt; streams = consumerMap.get(topic);</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">final</span> KafkaStream&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt; kafkaStream : streams)&#123;</span><br><span class="line">			<span class="comment">//因为指定了两个线程进行消费</span></span><br><span class="line">			<span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">				<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">					<span class="keyword">for</span>(MessageAndMetadata&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt; mm : kafkaStream)&#123;</span><br><span class="line">						String msg = <span class="keyword">new</span> String(mm.message());</span><br><span class="line">						System.out.println(msg);</span><br><span class="line">					&#125;</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;).start();</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Redis"><a href="#Redis" class="headerlink" title="Redis"></a>Redis</h2><p><a href="sparkStreaming笔记/Redis3.2.x单机安装(new).docx">详见文档</a></p>
<h3 id="Redis单机部署"><a href="#Redis单机部署" class="headerlink" title="Redis单机部署"></a>Redis单机部署</h3><ol>
<li><p>下载、上传redis-3.2.11.tar.gz到服务器并解压</p>
</li>
<li><p>进入到源码包中，编译并安装redis，预先下载gcc（c 的编译器）</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yum -y install gcc</span><br><span class="line">cd /usr/local/src/redis-3.2.11/</span><br><span class="line">make MALLOC=libc &amp;&amp; make install  #内存分配</span><br></pre></td></tr></table></figure>

<ol start="3">
<li><p>在所有机器的/usr/local/下创建一个redis目录，然后拷贝redis配置文件redis.conf到/usr/local/redis</p>
</li>
<li><p>修改所有机器的配置文件redis.conf</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">daemonize yes  #redis后台运行</span><br><span class="line"><span class="meta">#</span>cluster-enabled yes  #开集群后把注释去掉</span><br><span class="line">appendonly yes  #开启aof日志，它会每次写操作都记录一条日志</span><br><span class="line">bind 192.168.1.207</span><br></pre></td></tr></table></figure>

<ol start="5">
<li>启动所有的redis节点</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">redis-server /usr/local/redis/redis.conf  #make&amp;&amp;make install 就已经把redis添加到环境变量了</span><br></pre></td></tr></table></figure>

<ol start="6">
<li>使用命令行客户的连接redis</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">redis-cli -h 192.168.1.207</span><br><span class="line">redis-cli -p 6379 #-p为redis进程号，通过ps -ef | grep redis查询</span><br></pre></td></tr></table></figure>

<p>Tips:</p>
<ol>
<li>配置redis密码</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config set requirepass 123</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>再次连接的时候输入密码</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">auth 123</span><br></pre></td></tr></table></figure>

<h3 id="Redis-JavaAPI操作"><a href="#Redis-JavaAPI操作" class="headerlink" title="Redis-JavaAPI操作"></a>Redis-JavaAPI操作</h3><p>导入Redis依赖</p>
<ol>
<li>定义连接池参数，获取连接池方法</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> config = <span class="keyword">new</span> <span class="type">JedisPoolConfig</span>()</span><br><span class="line"><span class="comment">//最大连接数,</span></span><br><span class="line">config.setMaxTotal(<span class="number">20</span>)</span><br><span class="line"><span class="comment">//最大空闲连接数</span></span><br><span class="line">config.setMaxIdle(<span class="number">10</span>)</span><br><span class="line"><span class="comment">//当调用borrow Object方法时，是否进行有效性检查 --&gt;</span></span><br><span class="line">config.setTestOnBorrow(<span class="literal">true</span>)</span><br><span class="line"><span class="comment">//10000代表超时时间（10秒）</span></span><br><span class="line"><span class="keyword">val</span> pool = <span class="keyword">new</span> <span class="type">JedisPool</span>(config, <span class="string">"192.168.1.207"</span>, <span class="number">6379</span>, <span class="number">10000</span>, <span class="string">"123"</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getConnection</span></span>(): <span class="type">Jedis</span> = &#123;</span><br><span class="line">  pool.getResource</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conn = <span class="type">JedisConnectionPool</span>.getConnection()</span><br><span class="line">conn.set(<span class="string">"income"</span>, <span class="string">"1000"</span>)</span><br><span class="line"><span class="keyword">val</span> r1 = conn.get(<span class="string">"xiaoniu"</span>)</span><br><span class="line">conn.incrBy(<span class="string">"xiaoniu"</span>, <span class="number">-50</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.<span class="type">JavaConversions</span>._  <span class="comment">//导入隐式函数把java集合转成scala集合</span></span><br><span class="line"><span class="keyword">val</span> r = conn.keys(<span class="string">"*"</span>)  <span class="comment">//获得所有的key</span></span><br><span class="line"><span class="keyword">for</span> (p &lt;- r) &#123;	<span class="comment">//打印所有的key</span></span><br><span class="line">  println(p + <span class="string">" : "</span> + conn.get(p))</span><br><span class="line">&#125;</span><br><span class="line">conn.close()</span><br></pre></td></tr></table></figure>

<h2 id="Spark-On-Yarn"><a href="#Spark-On-Yarn" class="headerlink" title="Spark On Yarn"></a>Spark On Yarn</h2><p><a href="sparkStreaming笔记/spark-on-yarn.docx">详见文档</a></p>
<ol>
<li><p>环境</p>
<ul>
<li><p>安装hadoop：需要安装HDFS模块和YARN模块，HDFS必须安装，spark运行时要把jar包存放到HDFS上。</p>
</li>
<li><p>安装Spark：解压Spark安装程序到一台服务器上，修改spark-env.sh配置文件，spark程序将作为YARN的客户端用于提交任务</p>
<p>Tips：或者把HADOOP_CONF_DIR在/etc/profile里配置</p>
</li>
</ul>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_131</span><br><span class="line">export HADOOP_CONF_DIR=$HADOOP_HOME/etc/Hadoop</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><p>提交任务</p>
<ul>
<li><strong>cluster模式</strong></li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--driver-memory 1g \</span><br><span class="line">--executor-memory 1g \</span><br><span class="line">--executor-cores 2 \</span><br><span class="line">--queue default \  			#默认资源调度器</span><br><span class="line">lib/spark-examples*.jar \</span><br><span class="line">10						#参数</span><br></pre></td></tr></table></figure>

<ul>
<li><p>client模式</p>
<p>修改$HADOOP_HOME /etc/hadoop所有yarn节点的yarn-site.xml，在该文件中添加如下配置</p>
<p>修改内存检测机制</p>
</li>
</ul>
</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--deploy-mode client \ #其余不变</span><br></pre></td></tr></table></figure>

<ol start="3">
<li><p>两种模式的区别</p>
<ul>
<li><p>cluster模式：Driver程序在YARN中运行，应用的运行结果不能在客户端显示，所以最好运行那些将结果最终保存在外部存储介质（如HDFS、Redis、Mysql）而非stdout输出的应用程序，客户端的终端显示的仅是作为YARN的job的简单运行状况。</p>
</li>
<li><p>client模式：Driver运行在Client上，应用程序运行结果会在客户端显示，所有适合运行结果有输出的应用程序（如spark-shell）</p>
</li>
</ul>
</li>
</ol>
<h2 id="DStream知识补充"><a href="#DStream知识补充" class="headerlink" title="DStream知识补充"></a>DStream知识补充</h2><p>Spark Streaming是一个基于Spark Core之上的实时计算框架，可以从很多数据源消费数据并对数据进行处理，在Spark Streaing中有一个最基本的抽象叫DStream（代理），本质上就是一系列连续的RDD，DStream其实就是对RDD的封装。DStream可以任务是一个RDD的工厂，该DStream里面生产都是相同业务逻辑的RDD，只不过是RDD里面要读取数据的不相同</p>
<p>深入理解DStream：他是sparkStreaming中的一个最基本的抽象，代表了一下列连续的数据流，本质上是一系列连续的RDD，你对DStream进行操作，就是对RDD进行操作。DStream每隔一段时间生成一个RDD，你对DStream进行操作，本质上是对里面的对应时间的RDD进行操作。</p>
<p>DSteam和DStream之间存在依赖关系，在一个固定的时间点，对个存在依赖关系的DSrteam对应的RDD也存在依赖关系，每个一个固定的时间，其实生产了一个小的DAG，周期性的将生成的小DAG提交到集群中运行。</p>
<p><img src="sparkStreaming%E7%AC%94%E8%AE%B0/DStream%E8%AF%B4%E6%98%8E.png" alt="DStream抽象表示图"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/24/SPARK学习笔记/sparkSQL笔记/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chen Bin">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="cilibili">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/24/SPARK学习笔记/sparkSQL笔记/" itemprop="url">SparkSQL</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-07-24T11:31:41+08:00">
                2019-07-24
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="SparkSQL"><a href="#SparkSQL" class="headerlink" title="SparkSQL"></a>SparkSQL</h1><h2 id="spark2-x"><a href="#spark2-x" class="headerlink" title="spark2.x"></a>spark2.x</h2><h3 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a>执行流程</h3><p><strong>首先在pom里导入spark sql的依赖</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="1-创建sparkSession"><a href="#1-创建sparkSession" class="headerlink" title="1.创建sparkSession"></a>1.创建sparkSession</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val spark = SparkSession.builder()</span><br><span class="line">      .appName(&quot;SQLTest1&quot;)</span><br><span class="line">      .master(&quot;local[*]&quot;)</span><br><span class="line">      .getOrCreate()</span><br></pre></td></tr></table></figure>

<h4 id="2-创建RDD，对RDD数据整理"><a href="#2-创建RDD，对RDD数据整理" class="headerlink" title="2.创建RDD，对RDD数据整理"></a>2.创建RDD，对RDD数据整理</h4><p>方法一：用sparkContext创建RDD，再处理RDD数据</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建RDD</span></span><br><span class="line"><span class="keyword">val</span> lines: <span class="type">RDD</span>[<span class="type">String</span>] = spark.sparkContext.textFile(<span class="string">"hdfs://node-4:9000/person"</span>)</span><br><span class="line"><span class="comment">//将数据进行整理</span></span><br><span class="line"><span class="keyword">val</span> rowRDD: <span class="type">RDD</span>[<span class="type">Row</span>] = lines.map(line =&gt; &#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="type">Row</span>(id, name, age, fv)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<p><strong>方法二</strong>：用sparkSession直接创建DataSet[String] 类型，再用scalaAPI处理数据 （wordCount示例）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Dataset分布式数据集，是对RDD的进一步封装，是更加智能的RDD</span></span><br><span class="line"><span class="comment">//dataset只有一列，默认这列叫value</span></span><br><span class="line"><span class="comment">//调用sparksession.read.textFile("..")返回的是DataSet，调用ss.textFile("..")返回的是RDD</span></span><br><span class="line"><span class="keyword">val</span> lines: <span class="type">Dataset</span>[<span class="type">String</span>] = spark.read.textFile(<span class="string">"hdfs://node-4:9000/words"</span>)</span><br></pre></td></tr></table></figure>

<p>DataSet处理数据</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//整理数据(切分压平) wordCount</span></span><br><span class="line"><span class="comment">//DataSet调用了scala的API，需要导入隐式转换</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> words: <span class="type">Dataset</span>[<span class="type">String</span>] = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用DataSet的API（DSL）</span></span><br><span class="line"><span class="comment">//第一个count返回DataFrame（Transfomation），第二个count返回String（Action）</span></span><br><span class="line"><span class="keyword">val</span> r = words.groupBy($<span class="string">"value"</span> as <span class="string">"word"</span>).count().count()</span><br><span class="line"></span><br><span class="line"><span class="comment">//导入聚合函数</span></span><br><span class="line"><span class="comment">//import org.apache.spark.sql.functions._</span></span><br><span class="line"><span class="comment">//val counts = words.groupBy($"value".as("word")).agg(count("*") as "counts").orderBy($"counts" desc)</span></span><br></pre></td></tr></table></figure>

<h4 id="3-创建schema信息"><a href="#3-创建schema信息" class="headerlink" title="3.创建schema信息"></a>3.创建schema信息</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//结果类型，其实就是表头，用于描述DataFrame</span></span><br><span class="line"><span class="keyword">val</span> schema: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">List</span>(</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"id"</span>, <span class="type">LongType</span>, <span class="literal">true</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"name"</span>, <span class="type">StringType</span>, <span class="literal">true</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"age"</span>, <span class="type">IntegerType</span>, <span class="literal">true</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"fv"</span>, <span class="type">DoubleType</span>, <span class="literal">true</span>)</span><br><span class="line">))</span><br></pre></td></tr></table></figure>

<h4 id="4-创建DataFrame，关联schema信息"><a href="#4-创建DataFrame，关联schema信息" class="headerlink" title="4.创建DataFrame，关联schema信息"></a>4.创建DataFrame，关联schema信息</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建DataFrame</span></span><br><span class="line"><span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.createDataFrame(rowRDD, schema)</span><br></pre></td></tr></table></figure>

<h4 id="5-处理DataSet数据"><a href="#5-处理DataSet数据" class="headerlink" title="5.处理DataSet数据"></a>5.处理DataSet数据</h4><p>方法一：使用DataFrameAPI操作数据 （导入隐式函数）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> df2: <span class="type">Dataset</span>[<span class="type">Row</span>] = df.where($<span class="string">"fv"</span> &gt; <span class="number">98</span>).orderBy($<span class="string">"fv"</span> desc, $<span class="string">"age"</span> asc)</span><br></pre></td></tr></table></figure>

<p>方法二：使用SQL</p>
<h5 id="注册视图"><a href="#注册视图" class="headerlink" title="注册视图"></a>注册视图</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">words.createTempView(<span class="string">"v_wc"</span>)</span><br></pre></td></tr></table></figure>

<h5 id="执行SQL（Transformation，lazy）"><a href="#执行SQL（Transformation，lazy）" class="headerlink" title="执行SQL（Transformation，lazy）"></a>执行SQL（Transformation，lazy）</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">val</span> result: <span class="type">DataFrame</span> = spark.sql(</span><br><span class="line">    <span class="string">"SELECT value word, COUNT(*) counts </span></span><br><span class="line"><span class="string">    FROM v_wc </span></span><br><span class="line"><span class="string">    GROUP BY word </span></span><br><span class="line"><span class="string">    ORDER BY counts DESC"</span>)</span><br><span class="line"><span class="comment">//执行Action</span></span><br><span class="line">result.show()	</span><br><span class="line"></span><br><span class="line"><span class="comment">//往其他存储软件里写 ==================================存疑=========================</span></span><br><span class="line">result.foreachPartition(iter:(<span class="type">Iterator</span>[<span class="type">Row</span>]) =&gt;&#123;</span><br><span class="line">	&#125;)</span><br></pre></td></tr></table></figure>

<h4 id="6-关闭sparkSession连接"><a href="#6-关闭sparkSession连接" class="headerlink" title="6.关闭sparkSession连接"></a>6.关闭sparkSession连接</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="spark-join-连接表"><a href="#spark-join-连接表" class="headerlink" title="spark join 连接表"></a>spark join 连接表</h3><h4 id="1-创建两个DataFrame"><a href="#1-创建两个DataFrame" class="headerlink" title="1.创建两个DataFrame"></a>1.创建两个DataFrame</h4><ul>
<li>表1</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//对Dataset进行RDD的操作要导入隐式转换  </span></span><br><span class="line"><span class="comment">//这里是spark是SparkSession的一个实例</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> lines: <span class="type">Dataset</span>[<span class="type">String</span>] = spark.createDataset(<span class="type">List</span>(<span class="string">"1,laozhoa,china"</span>, <span class="string">"2,laoduan,usa"</span>, <span class="string">"3,laoyang,jp"</span>))</span><br><span class="line"><span class="comment">//对数据进行整理</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> tpDs: <span class="type">Dataset</span>[(<span class="type">Long</span>, <span class="type">String</span>, <span class="type">String</span>)] = lines.map(line =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> fields = line.split(<span class="string">","</span>)</span><br><span class="line">  <span class="keyword">val</span> id = fields(<span class="number">0</span>).toLong</span><br><span class="line">  <span class="keyword">val</span> name = fields(<span class="number">1</span>)</span><br><span class="line">  <span class="keyword">val</span> nationCode = fields(<span class="number">2</span>)</span><br><span class="line">  (id, name, nationCode)</span><br><span class="line">&#125;)</span><br><span class="line"><span class="keyword">val</span> df1 = tpDs.toDF(<span class="string">"id"</span>, <span class="string">"name"</span>, <span class="string">"nation"</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>表2</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">val</span> nations: <span class="type">Dataset</span>[<span class="type">String</span>] = spark.createDataset(<span class="type">List</span>(<span class="string">"china,中国"</span>, <span class="string">"usa,美国"</span>))</span><br><span class="line"><span class="comment">//对数据进行整理</span></span><br><span class="line"><span class="keyword">val</span> ndataset: <span class="type">Dataset</span>[(<span class="type">String</span>, <span class="type">String</span>)] = nations.map(l =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> fields = l.split(<span class="string">","</span>)</span><br><span class="line">  <span class="keyword">val</span> ename = fields(<span class="number">0</span>)</span><br><span class="line">  <span class="keyword">val</span> cname = fields(<span class="number">1</span>)</span><br><span class="line">  (ename, cname)</span><br><span class="line">&#125;)</span><br><span class="line"><span class="keyword">val</span> df2 = ndataset.toDF(<span class="string">"ename"</span>,<span class="string">"cname"</span>)</span><br></pre></td></tr></table></figure>

<h4 id="2-对两个DataFrame操作"><a href="#2-对两个DataFrame操作" class="headerlink" title="2.对两个DataFrame操作"></a>2.对两个DataFrame操作</h4><h5 id="方式一：SQL"><a href="#方式一：SQL" class="headerlink" title="方式一：SQL"></a>方式一：SQL</h5><p>1.创建两个视图（方式一：SQL）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df1.createTempView(<span class="string">"v_users"</span>)</span><br><span class="line">df2.createTempView(<span class="string">"v_nations"</span>)</span><br></pre></td></tr></table></figure>

<p>2.写SQL</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> r: <span class="type">DataFrame</span> = spark.sql(</span><br><span class="line">    <span class="string">"SELECT name, cname </span></span><br><span class="line"><span class="string">    FROM v_users JOIN v_nations </span></span><br><span class="line"><span class="string">    ON nation = ename"</span>)</span><br><span class="line">r.show()</span><br></pre></td></tr></table></figure>

<h5 id="方式二：DataFrameAPI"><a href="#方式二：DataFrameAPI" class="headerlink" title="方式二：DataFrameAPI"></a>方式二：DataFrameAPI</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> r = df1.join(df2, $<span class="string">"nation"</span> === $<span class="string">"ename"</span>, <span class="string">"left_outer"</span>)</span><br><span class="line">r.show()</span><br></pre></td></tr></table></figure>

<h5 id="补充：三种spark-join-方式（详见博客）"><a href="#补充：三种spark-join-方式（详见博客）" class="headerlink" title="补充：三种spark join 方式（详见博客）"></a>补充：三种spark join 方式（<a href="https://www.cnblogs.com/duodushuduokanbao/p/9911256.html" target="_blank" rel="noopener">详见博客</a>）</h5><ol>
<li><p><strong>Broadcast Hash Join</strong>（大表与小表join）(spark默认join方式)</p>
<p>步骤：</p>
<ol>
<li>broadcast阶段：将小表广播到所有的executor上，广播的算法有很多，最简单的是先发给driver，driver再统一分发给所有的executor，要不就是基于bittorrete的p2p思路；</li>
<li>hash join阶段：在每个executor上执行 hash join，小表构建为hash table，大表的分区数据匹配hash table中的数据；</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> result: <span class="type">DataFrame</span> = df1.join(df2, $<span class="string">"id"</span> === $<span class="string">"aid"</span>)</span><br><span class="line"><span class="keyword">val</span> result: <span class="type">DataFrame</span> = df1.join(broadcast(df2), $<span class="string">"id"</span> === $<span class="string">"aid"</span>)<span class="comment">//两者相同</span></span><br></pre></td></tr></table></figure>

<p>条件：</p>
<ul>
<li>被广播的表需要小于spark.sql.autoBroadcastJoinThreshold所配置的信息，默认是10M；</li>
<li>基表不能被广播，比如left outer join时，只能广播右表。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.conf.set(<span class="string">"spark.sql.autoBroadcastJoinThreshold"</span>, <span class="number">1024</span>*<span class="number">1024</span>*<span class="number">10</span>) <span class="comment">// 设置为10M</span></span><br></pre></td></tr></table></figure>

<p>先将小表在driver缓存内存，再broadCast，会对小表的扫描localTablesScan变为InMemoryScan</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df2.cache().count()</span><br><span class="line">df1.join(broadcast(df2), $<span class="string">"id"</span> === $<span class="string">"aid"</span>)</span><br></pre></td></tr></table></figure>



</li>
</ol>
<ol start="2">
<li><p><strong>Shuffle Hash Join</strong>（大表与较小表join）</p>
<p>步骤：</p>
<ol>
<li>对两张表分别按照join key进行重分区(分区函数相同的时候，相同的相同分区中的key一定是相同的)，即shuffle，目的是为了让相同join key的记录分到对应的分区中；</li>
<li>对对应分区中的数据进行join，此处先将小表分区构建为一个hash表，然后根据大表中记录的join key的hash值拿来进行匹配，即每个节点山单独执行hash算法。</li>
</ol>
<p>条件：</p>
<ul>
<li><p>分区的平均大小不超过spark.sql.autoBroadcastJoinThreshold所配置的值，默认是10M </p>
</li>
<li><p>基表不能被广播，比如left outer join时，只能广播右表</p>
</li>
<li><p>一侧的表要明显小于另外一侧，小的一侧将被广播（明显小于的定义为3倍小，此处为经验值）</p>
</li>
</ul>
</li>
<li><p><strong>Sort Merge Join</strong>（两个大表进行join）</p>
<p>步骤：</p>
<ol>
<li><p>shuffle阶段：将两张大表根据join key进行重新分区，两张表数据会分布到整个集群，以便分布式并行处理；</p>
</li>
<li><p>sort阶段：对单个分区节点的两表数据，分别进行排序；</p>
</li>
<li><p>merge阶段：对排好序的两张分区表数据执行join操作。join操作很简单，分别遍历两个有序序列，碰到相同join key就merge输出，否则取更小一边</p>
</li>
</ol>
<p>条件：</p>
<ul>
<li>两个大表</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.conf.set(<span class="string">"spark.sql.join.preferSortMergeJoin"</span>, <span class="literal">true</span>)<span class="comment">//方法一</span></span><br><span class="line">spark.sql.autoBroadcastJoinThreshold=<span class="number">-1</span><span class="comment">//方法二</span></span><br></pre></td></tr></table></figure>



</li>
</ol>
<hr>
<h3 id="自定义函数-UDF、UDAF"><a href="#自定义函数-UDF、UDAF" class="headerlink" title="自定义函数 UDF、UDAF"></a>自定义函数 UDF、UDAF</h3><h4 id="UDF-（user-defined-function-1映射到1）"><a href="#UDF-（user-defined-function-1映射到1）" class="headerlink" title="UDF （user-defined function 1映射到1）"></a>UDF （user-defined function 1映射到1）</h4><ol>
<li>自定义函数并注册</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//该函数的功能是（输入一个IP地址对应的十进制，返回一个省份名称）</span></span><br><span class="line">spark.udf.register(<span class="string">"ip2Province"</span>, (ipNum: <span class="type">Long</span>) =&gt; &#123; <span class="comment">//传入IP:Long数据</span></span><br><span class="line">      ...<span class="comment">//处理逻辑见 “统计IP示例”</span></span><br><span class="line">      province <span class="comment">//返回省份字符串</span></span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>调用</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">"SELECT ip2Province(ip_num) province FROM v_log "</span>).show()</span><br></pre></td></tr></table></figure>

<h4 id="UDAF-（user-defined-aggregate-function-n映射到1）"><a href="#UDAF-（user-defined-aggregate-function-n映射到1）" class="headerlink" title="UDAF （user-defined-aggregate function n映射到1）"></a>UDAF （user-defined-aggregate function n映射到1）</h4><h5 id="1-定义聚合函数"><a href="#1-定义聚合函数" class="headerlink" title="1. 定义聚合函数"></a>1. 定义聚合函数</h5><p>$$<br>函数功能：   \sqrt[n]{x_1<em>x_2</em>…*x_n}<br>$$</p>
<ul>
<li>继承UserDefinedAggregateFunction，实现8个方法</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GeoMean</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span></span></span><br></pre></td></tr></table></figure>

<ul>
<li>输入数据的类型</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">List</span>(</span><br><span class="line">  <span class="type">StructField</span>(<span class="string">"value"</span>, <span class="type">DoubleType</span>)</span><br><span class="line">))</span><br></pre></td></tr></table></figure>

<ul>
<li>产生中间结果的数据类型</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">List</span>(</span><br><span class="line">  <span class="comment">//相乘之后返回的积</span></span><br><span class="line">  <span class="type">StructField</span>(<span class="string">"product"</span>, <span class="type">DoubleType</span>),</span><br><span class="line">  <span class="comment">//参与运算数字的个数</span></span><br><span class="line">  <span class="type">StructField</span>(<span class="string">"counts"</span>, <span class="type">LongType</span>)</span><br><span class="line">))</span><br></pre></td></tr></table></figure>

<ul>
<li>最终返回的结果类型</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">DoubleType</span></span><br></pre></td></tr></table></figure>

<ul>
<li>确保一致性 一般用true</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<ul>
<li>指定初始值</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">//相乘的初始值</span></span><br><span class="line">  buffer(<span class="number">0</span>) = <span class="number">1.0</span></span><br><span class="line">  <span class="comment">//参与运算数字的个数的初始值</span></span><br><span class="line">  buffer(<span class="number">1</span>) = <span class="number">0</span>L</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>每有一条数据参与运算就更新一下中间结果(update相当于在每一个分区中的运算)</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">//每有一个数字参与运算就进行相乘（包含中间结果）</span></span><br><span class="line">  buffer(<span class="number">0</span>) = buffer.getDouble(<span class="number">0</span>) * input.getDouble(<span class="number">0</span>)</span><br><span class="line">  <span class="comment">//参与运算数据的个数也有更新</span></span><br><span class="line">  buffer(<span class="number">1</span>) = buffer.getLong(<span class="number">1</span>) + <span class="number">1</span>L</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>全局聚合</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">//每个分区计算的结果进行相乘</span></span><br><span class="line">  buffer1(<span class="number">0</span>) =  buffer1.getDouble(<span class="number">0</span>) * buffer2.getDouble(<span class="number">0</span>)</span><br><span class="line">  <span class="comment">//每个分区参与预算的中间结果进行相加</span></span><br><span class="line">  buffer1(<span class="number">1</span>) =  buffer1.getLong(<span class="number">1</span>) + buffer2.getLong(<span class="number">1</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>计算最终的结果</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Double</span> = &#123;</span><br><span class="line">  math.pow(buffer.getDouble(<span class="number">0</span>), <span class="number">1.</span>toDouble / buffer.getLong(<span class="number">1</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="2-调用聚合函数"><a href="#2-调用聚合函数" class="headerlink" title="2. 调用聚合函数"></a>2. 调用聚合函数</h5><ol>
<li>使用SQL</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> geomean = <span class="keyword">new</span> <span class="type">GeoMean</span></span><br><span class="line"><span class="keyword">val</span> range: <span class="type">Dataset</span>[<span class="type">Long</span>] = spark.range(<span class="number">1</span>, <span class="number">11</span>)<span class="comment">//定义1~11的单列数据</span></span><br><span class="line"><span class="comment">//注册函数</span></span><br><span class="line">spark.udf.register(<span class="string">"gm"</span>, geomean)</span><br><span class="line"><span class="comment">//将range这个Dataset[Long]注册成视图</span></span><br><span class="line">range.createTempView(<span class="string">"v_range"</span>)</span><br><span class="line"><span class="keyword">val</span> result = spark.sql(<span class="string">"SELECT gm(id) result FROM v_range"</span>)<span class="comment">//调用聚合函数</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li>DSL风格</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//使用DSL风格不用注册和建视图</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> result = range.groupBy().agg(geomean($<span class="string">"id"</span>).as(<span class="string">"geomean"</span>)) <span class="comment">//这里可以不用掉groupBy</span></span><br></pre></td></tr></table></figure>

<h4 id="UDTF"><a href="#UDTF" class="headerlink" title="UDTF"></a>UDTF</h4><p>输入一行，返回多行（hive）一对多    spark SQL中没有UDTF，spark中用flatMap即可实现该功能</p>
<h3 id="统计IP地址示例"><a href="#统计IP地址示例" class="headerlink" title="统计IP地址示例"></a>统计IP地址示例</h3><h4 id="初级版本：连接两张表查询"><a href="#初级版本：连接两张表查询" class="headerlink" title="初级版本：连接两张表查询"></a>初级版本：连接两张表查询</h4><ol>
<li>创建sparkSession</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">"JoinTest"</span>)</span><br><span class="line">      .master(<span class="string">"local[*]"</span>)</span><br><span class="line">      .getOrCreate()</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>创建”IP-地区规则表”&amp;”IP记录表”的DataFrame</li>
</ol>
<ul>
<li>IP-地区规则表</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//取到HDFS中的ip规则</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._                                   <span class="comment">//导入隐式参数因为用了Map方法</span></span><br><span class="line"><span class="keyword">val</span> rulesLines:<span class="type">Dataset</span>[<span class="type">String</span>] = spark.read.textFile(args(<span class="number">0</span>))</span><br><span class="line"><span class="comment">//整理ip规则数据()</span></span><br><span class="line"><span class="keyword">val</span> ruleDataFrame: <span class="type">DataFrame</span> = rulesLines.map(line =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> fields = line.split(<span class="string">"[|]"</span>)</span><br><span class="line">  <span class="keyword">val</span> startNum = fields(<span class="number">2</span>).toLong</span><br><span class="line">  <span class="keyword">val</span> endNum = fields(<span class="number">3</span>).toLong</span><br><span class="line">  <span class="keyword">val</span> province = fields(<span class="number">6</span>)</span><br><span class="line">  (startNum, endNum, province)</span><br><span class="line">&#125;).toDF(<span class="string">"snum"</span>, <span class="string">"enum"</span>, <span class="string">"province"</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>IP记录表</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建RDD，读取ip访问日志</span></span><br><span class="line"><span class="keyword">val</span> accessLines: <span class="type">Dataset</span>[<span class="type">String</span>] = spark.read.textFile(args(<span class="number">1</span>))</span><br><span class="line"><span class="comment">//整理数据</span></span><br><span class="line"><span class="keyword">val</span> ipDataFrame: <span class="type">DataFrame</span> = accessLines.map(log =&gt; &#123;</span><br><span class="line">  <span class="comment">//将log日志的每一行进行切分</span></span><br><span class="line">  <span class="keyword">val</span> fields = log.split(<span class="string">"[|]"</span>)</span><br><span class="line">  <span class="keyword">val</span> ip = fields(<span class="number">1</span>)</span><br><span class="line">  <span class="comment">//将ip转换成十进制</span></span><br><span class="line">  <span class="keyword">val</span> ipNum = <span class="type">MyUtils</span>.ip2Long(ip) <span class="comment">//将111.11.11.11整理成Long型数据</span></span><br><span class="line">  ipNum</span><br><span class="line">&#125;).toDF(<span class="string">"ip_num"</span>)</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>创建两张视图</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ruleDataFrame.createTempView(<span class="string">"v_rules"</span>)</span><br><span class="line">ipDataFrame.createTempView(<span class="string">"v_ips"</span>)</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>执行SQL</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> r = spark.sql(</span><br><span class="line"><span class="string">"SELECT province, count(*) counts </span></span><br><span class="line"><span class="string">FROM v_ips JOIN v_rules </span></span><br><span class="line"><span class="string">ON (ip_num &gt;= snum AND ip_num &lt;= enum)         </span></span><br><span class="line"><span class="string">GROUP BY province ORDER BY counts DESC"</span>        </span><br><span class="line">).show()										<span class="comment">//两表字段非等值内连接</span></span><br></pre></td></tr></table></figure>

<ol start="5">
<li>关闭数据流</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.stop</span><br></pre></td></tr></table></figure>

<h4 id="高级版本：缓存广播小表-UDF"><a href="#高级版本：缓存广播小表-UDF" class="headerlink" title="高级版本：缓存广播小表,UDF"></a>高级版本：缓存广播小表,UDF</h4><ol>
<li><p>创建sparkSession</p>
</li>
<li><p>整理IP规则为Array[(Long, Long, String)]，不再创建表了</p>
</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> rulesLines:<span class="type">Dataset</span>[<span class="type">String</span>] = spark.read.textFile(args(<span class="number">0</span>))</span><br><span class="line"><span class="comment">//整理ip规则数据()</span></span><br><span class="line"><span class="keyword">val</span> rluesDataset = rulesLines.map(line =&gt; &#123;</span><br><span class="line">	...</span><br><span class="line">    (startNum, endNum, province)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>收集ip规则到Driver端，并且广播</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rulesInDriver: <span class="type">Array</span>[(<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">String</span>)] = rluesDataset.collect() <span class="comment">//已缓存内存</span></span><br><span class="line"><span class="comment">//广播(必须使用sparkcontext)</span></span><br><span class="line"><span class="comment">//将广播变量的引用返回到Driver端</span></span><br><span class="line"><span class="keyword">val</span> broadcastRef: <span class="type">Broadcast</span>[<span class="type">Array</span>[(<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">String</span>)]] = spark.sparkContext.broadcast(rulesInDriver)</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>读IP记录创建dataFrame和视图</li>
<li>定义一个自定义函数（UDF），并注册</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//该函数的功能是（输入一个IP地址对应的十进制，返回一个省份名称）</span></span><br><span class="line">spark.udf.register(<span class="string">"ip2Province"</span>, (ipNum: <span class="type">Long</span>) =&gt; &#123;</span><br><span class="line">  <span class="comment">//查找ip规则（事先已经广播了，已经在Executor中了）</span></span><br><span class="line">  <span class="comment">//函数的逻辑是在Executor中执行的，怎样获取ip规则的对应的数据呢？</span></span><br><span class="line">  <span class="comment">//使用广播变量的引用，就可以获得</span></span><br><span class="line">  <span class="keyword">val</span> ipRulesInExecutor: <span class="type">Array</span>[(<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">String</span>)] = broadcastRef.value</span><br><span class="line">  <span class="comment">//根据IP地址对应的十进制查找省份名称</span></span><br><span class="line">  <span class="keyword">val</span> index = <span class="type">MyUtils</span>.binarySearch(ipRulesInExecutor, ipNum)</span><br><span class="line">  <span class="keyword">var</span> province = <span class="string">"未知"</span></span><br><span class="line">  <span class="keyword">if</span>(index != <span class="number">-1</span>) &#123;</span><br><span class="line">    province = ipRulesInExecutor(index)._3</span><br><span class="line">  &#125;</span><br><span class="line">  province</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<p>6.SQL里调用自定义函数</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(</span><br><span class="line">    <span class="string">"SELECT ip2Province(ip_num) province, COUNT(*) counts </span></span><br><span class="line"><span class="string">	FROM v_log </span></span><br><span class="line"><span class="string">	GROUP BY province </span></span><br><span class="line"><span class="string">	ORDER BY counts DESC"</span>).show()</span><br></pre></td></tr></table></figure>

<h3 id="最受欢迎老师TOPN"><a href="#最受欢迎老师TOPN" class="headerlink" title="最受欢迎老师TOPN"></a>最受欢迎老师TOPN</h3><h4 id="hive窗口分析函数"><a href="#hive窗口分析函数" class="headerlink" title="hive窗口分析函数"></a>hive窗口分析函数</h4><ul>
<li><p>row_number() over(partition by subject order by counts desc)    增加行号</p>
</li>
<li><p>rank() over(order by counts desc)       增加序列号（1，2，2，4…）</p>
</li>
<li><p>dense_rank() over(order by counts desc)         增加序列号（1，2，2，3….）</p>
</li>
</ul>
<h4 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h4><ol>
<li>获取sparkSessioni</li>
<li>创建dataFrame   (subject, teacher).toDF(“subject”, “teacher”)</li>
<li>创建视图.createTempView(“v_sub_teacher”)</li>
<li><strong>SQL创建DataFrame</strong>（根据v_sub_teacher表创建多出counts列的新表）</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//SQL创建新的DataFrame，该学科下的老师的访问次数</span></span><br><span class="line"><span class="keyword">val</span> temp1: <span class="type">DataFrame</span> = spark.sql(</span><br><span class="line"><span class="string">"SELECT subject, teacher, count(*) counts</span></span><br><span class="line"><span class="string">FROM v_sub_teacher </span></span><br><span class="line"><span class="string">GROUP BY subject, teacher"</span>)</span><br><span class="line"><span class="comment">//创建新视图</span></span><br><span class="line">temp1.createTempView(<span class="string">"v_temp_sub_teacher_counts"</span>)</span><br></pre></td></tr></table></figure>

<ol start="5">
<li>对新视图执行SQL</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//scala用法：字符串前加s，""内直接用$取参数值</span></span><br><span class="line"><span class="keyword">val</span> temp2 = spark.sql(s</span><br><span class="line"><span class="string">"SELECT *, row_number() over(order by counts desc) g_rk </span></span><br><span class="line"><span class="string">FROM </span></span><br><span class="line"><span class="string">(SELECT subject, teacher, counts, </span></span><br><span class="line"><span class="string">row_number() over(partition by subject order by counts desc) sub_rk</span></span><br><span class="line"><span class="string">FROM v_temp_sub_teacher_counts) temp2 </span></span><br><span class="line"><span class="string">WHERE sub_rk &lt;= $topN"</span>)  <span class="comment">//取出前topN的数据</span></span><br></pre></td></tr></table></figure>

<h3 id="读存多种数据源"><a href="#读存多种数据源" class="headerlink" title="读存多种数据源"></a>读存多种数据源</h3><h4 id="JDBC"><a href="#JDBC" class="headerlink" title="JDBC"></a>JDBC</h4><p><strong>读取数据</strong></p>
<ol>
<li>定义Property对象保存连接信息</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> props = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">props.put(<span class="string">"user"</span>,<span class="string">"root"</span>)</span><br><span class="line">props.put(<span class="string">"password"</span>,<span class="string">"root"</span>)</span><br><span class="line">props.put(<span class="string">"driver"</span>,<span class="string">"com.mysql.jdbc.Driver"</span>) <span class="comment">//导入mysql-connector依赖</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li>调用访问jdbc依赖</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data:<span class="type">DataFrame</span> = spark.read.jdbc(<span class="string">"jdbc:mysql://localhost:3306/fe"</span>,<span class="string">"job"</span>,props)</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>打印表信息</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data.printSchema() <span class="comment">//表头信息</span></span><br><span class="line">data.show() <span class="comment">//表信息</span></span><br></pre></td></tr></table></figure>

<p><strong>写入数据</strong></p>
<p><strong>spark.write.mode(“xx”)类型：</strong>Append、ErrorIfExists、Ignore、OverWrite</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> props = <span class="keyword">new</span> <span class="type">Properties</span>() <span class="comment">//写入数据不用jdbcDriver</span></span><br><span class="line">props.put(<span class="string">"user"</span>,<span class="string">"root"</span>)</span><br><span class="line">props.put(<span class="string">"password"</span>,<span class="string">"root"</span>)</span><br><span class="line">reslut.write.mode(<span class="string">"ignore"</span>).jdbc(</span><br><span class="line">    <span class="string">"jdbc:mysql://localhost:3306/bigdata"</span>, <span class="string">"logs1"</span>, props)</span><br></pre></td></tr></table></figure>

<p><strong>备注</strong>：写入text注意</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//DataFrame保存成text时(只能保存一列String类型的数据)!!!!</span></span><br><span class="line">reslut.write.text(<span class="string">"/Users/zx/Desktop/text"</span>)</span><br></pre></td></tr></table></figure>

<h4 id="JSON"><a href="#JSON" class="headerlink" title="JSON"></a>JSON</h4><p><strong>读取数据</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> jsons: <span class="type">DataFrame</span> = spark.read.json(<span class="string">"/Users/zx/Desktop/json"</span>)</span><br></pre></td></tr></table></figure>

<p><strong>写入数据</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reslut.write.json(<span class="string">"/Users/zx/Desktop/json"</span>)</span><br></pre></td></tr></table></figure>

<h4 id="CSV"><a href="#CSV" class="headerlink" title="CSV"></a>CSV</h4><p><strong>读取数据</strong></p>
<p>读取出的数据scehma信息为_c1,_c2…_cn  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> csv: <span class="type">DataFrame</span> = spark.read.csv(<span class="string">"/Users/zx/Desktop/csv"</span>)</span><br></pre></td></tr></table></figure>

<p><strong>存入数据</strong></p>
<p>存入数据没有表头信息</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reslut.write.csv(<span class="string">"/Users/zx/Desktop/csv"</span>)</span><br></pre></td></tr></table></figure>

<h4 id="Parquet"><a href="#Parquet" class="headerlink" title="Parquet"></a>Parquet</h4><p>Parquet：面向分析型业务的列式存储格式、spark可以只读取其中某几列</p>
<p><strong>读取数据</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> parquetLine: <span class="type">DataFrame</span> = spark.read.parquet(<span class="string">"/Users/zx/Desktop/parquet"</span>)</span><br></pre></td></tr></table></figure>

<p><strong>写入数据</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reslut.write.parquet(<span class="string">"hdfs://node-4:9000/parquet"</span>)</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="sparkSQL整合hive"><a href="#sparkSQL整合hive" class="headerlink" title="sparkSQL整合hive"></a>sparkSQL整合hive</h3><p>跟hive没太的关系，就是使用了hive的标准（HQL， 元数据库、UDF、序列化、反序列化机制</p>
<p>真正要计算的数据是保存在HDFS中，mysql这个元数据库，保存的是hive表的描述信息，描述了有哪些database、table、以及表有多少列，每一列是什么类型，还要描述表的数据保存在hdfs的什么位置</p>
<p><strong>hive的元数据库的功能：</strong><br>    建立了一种映射关系，执行HQL时，先到MySQL元数据库中查找描述信息，然后根据描述信息生成任务，然后将任务下发到spark集群中执行</p>
<p><strong>pom中导入spark-hive依赖</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="方式一：spark-sql-shell-配置启动"><a href="#方式一：spark-sql-shell-配置启动" class="headerlink" title="方式一：spark-sql shell 配置启动"></a>方式一：spark-sql shell 配置启动</h4><ol>
<li>安装MySQL（hive的元数据库）并创建一个普通用户，并且授权</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. CREATE USER &apos;xiaoniu&apos;@&apos;%&apos; IDENTIFIED BY &apos;123568&apos;; </span><br><span class="line">2. GRANT ALL PRIVILEGES ON hivedb.* TO &apos;xiaoniu&apos;@&apos;%&apos; IDENTIFIED BY &apos;123568&apos; </span><br><span class="line">WITH GRANT OPTION;</span><br><span class="line">3. FLUSH PRIVILEGES;</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>在spark的conf目录下添加一个hive-site.xml（创建一个hive的配置文件）</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://node-6:3306/hivedb?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">   <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>bigdata<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>123568<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ol start="3">
<li><p>上传一个mysql连接驱动（sparkSubmit也要连接MySQL，获取元数据信息）</p>
<p>启动spark-sql shell 命令：（如果mysql驱动在spark/lib下就不用跟driver-class-path）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./spark-sql --master spark://node-4:7077,node-5:7077 --driver-class-path /home/xiaoniu/mysql-connector-java-5.1.7-bin.jar</span><br></pre></td></tr></table></figure>

<p>spark-sql 执行 SQL文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>spark sql 整合hive，将hive的sql写在一个文件中执行（用-f这个参数）</span><br><span class="line">spark-sql --master spark://node-4:7077,node-5:7077 -f hive-sqls.sql</span><br></pre></td></tr></table></figure>
</li>
<li><p>sparkSQL会在mysql上创建一个database(hivedb保存元数据信息)</p>
<p><strong>注意！：</strong><u>需要手动改一下</u>DBS表中的DB_LOCATION_UIR(由本地地址file:/)改成hdfs的地址(hdfs://master:9000/xx)</p>
<p><strong>补充</strong>：因为在建表和导入本地数据时虽然可以成功，但spark-sql shell在执行查询等语句时是多台机器在本地查找数据计算，只有本地的机器有数据，其他机器找不到数据从而出错。</p>
</li>
<li><p>要在/etc/profile中配置一个环节变量(让sparkSQL知道hdfs在哪里，其实就是namenode在哪里)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">exprot HADOOP_CONF_DIR=$HADOOP_HOME #指定HADOOP配置文件路径</span><br></pre></td></tr></table></figure>
</li>
<li><p>重新启动SparkSQL的命令行</p>
</li>
</ol>
<h4 id="方式二：IDE中执行"><a href="#方式二：IDE中执行" class="headerlink" title="方式二：IDE中执行"></a>方式二：IDE中执行</h4><ol>
<li>开启spark对hive的支持</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">SparkSession</span>.builder().enableHiveSupport()....</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><p>在resource文件夹下添加hive-site.xml<strong>（非高可用）</strong></p>
<p>在本地运行需要元数据库配置文件信息，如果在集群运行就不需要</p>
<p><strong>备注</strong>：如果配置了高可用hdfs（两个namenode，DBS表中的DB_LOCATION_UIR的hdfs路径例如配置为hdfs://ns/xxx，提交运行时找不到hdfs://ns地址，如果是hdfs://master:9000就可以），则还需要添加core-site.xml、hdfs-site.xml</p>
</li>
<li><p>实行sparkSQL（执行HIVE）</p>
</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> result: <span class="type">DataFrame</span> = spark.sql(<span class="string">"SELECT * FROM t_boy ORDER BY fv DESC"</span>)</span><br><span class="line"><span class="keyword">val</span> sql: <span class="type">DataFrame</span> = spark.sql(<span class="string">"CREATE TABLE niu (id bigint, name string)"</span>)</span><br><span class="line">result.show()</span><br><span class="line">sql.show</span><br></pre></td></tr></table></figure>

<h4 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h4><ul>
<li>非高可用：</li>
</ul>
<p>core-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://master:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>hdfs-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/root/hdpdata/name/<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/root/hdpdata/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>slave1:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li><p>高可用：</p>
<p>见文件：<a href="sparkSQL笔记/conf/core-site.xml">core-site.xml</a>、<a href="sparkSQL笔记/conf/hdfs-site.xml">hdfs-site.xml</a></p>
</li>
</ul>
<h2 id="spark1-x"><a href="#spark1-x" class="headerlink" title="spark1.x"></a>spark1.x</h2><h3 id="创建方式一"><a href="#创建方式一" class="headerlink" title="创建方式一"></a>创建方式一</h3><p>1.创建SparkContext</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//提交的这个程序可以连接到Spark集群中</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"SQLDemo1"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line"><span class="comment">//创建SparkSQL的连接（程序执行的入口）</span></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br></pre></td></tr></table></figure>

<p>2.创建SQLContext</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//sparkContext不能创建特殊的RDD（DataFrame）</span></span><br><span class="line"><span class="comment">//将SparkContext包装进而增强</span></span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br></pre></td></tr></table></figure>

<p>3.创建一个类（建议case class不用new和序列化），并定义类的成员变量</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Boy</span>(<span class="params">id: <span class="type">Long</span>, name: <span class="type">String</span>, age: <span class="type">Int</span>, fv: <span class="type">Double</span></span>)</span></span><br></pre></td></tr></table></figure>

<p>4.创建RDD,将数据进行整理成class对象（关联class，将非结构化数据转换成结构化数据）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建特殊的RDD（DataFrame），就是有schema信息的RDD</span></span><br><span class="line"><span class="comment">//先有一个普通的RDD，然后在关联上schema，进而转成DataFrame</span></span><br><span class="line"><span class="keyword">val</span> lines = sc.textFile(<span class="string">"hdfs://node-4:9000/person"</span>)</span><br><span class="line"><span class="comment">//将数据进行整理</span></span><br><span class="line"><span class="keyword">val</span> boyRDD: <span class="type">RDD</span>[<span class="type">Boy</span>] = lines.map(line =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> fields = line.split(<span class="string">","</span>)</span><br><span class="line">    <span class="keyword">val</span> id = fields(<span class="number">0</span>).toLong</span><br><span class="line">	...</span><br><span class="line">    <span class="type">Boy</span>(id, name, age, fv)</span><br><span class="line">&#125;)</span><br><span class="line"><span class="comment">//该RDD装的是Boy类型的数据，有了shcma信息，但是还是一个RDD</span></span><br></pre></td></tr></table></figure>

<p>6.将RDD转换成DataFrame（导入隐式转换），显式调用toDF方法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//将RDD转换成DataFrame</span></span><br><span class="line"><span class="comment">//导入隐式转换</span></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"><span class="keyword">val</span> bdf: <span class="type">DataFrame</span> = boyRDD.toDF</span><br><span class="line"><span class="comment">//变成DF后就可以使用两种API进行编程了</span></span><br></pre></td></tr></table></figure>

<p>7.将DataFrame注册成临时表（SQL方法）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//把DataFrame先注册临时表</span></span><br><span class="line">bdf.registerTempTable(<span class="string">"t_boy"</span>)</span><br></pre></td></tr></table></figure>

<p>8.书写SQL（Transformation）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//书写SQL（SQL方法应其实是Transformation）</span></span><br><span class="line"><span class="keyword">val</span> result: <span class="type">DataFrame</span> = sqlContext.sql(<span class="string">"SELECT * FROM t_boy ORDER BY fv desc, age asc"</span>)</span><br></pre></td></tr></table></figure>

<p>9.执行Action</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">result.show()</span><br><span class="line">sc.stop()</span><br></pre></td></tr></table></figure>

<h3 id="创建方式二"><a href="#创建方式二" class="headerlink" title="创建方式二"></a>创建方式二</h3><p>1.创建SparkContext<br>2.创建SQLContext<br>3.创建RDD，将数据整理成Row对象</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rowRDD: <span class="type">RDD</span>[<span class="type">Row</span>] = lines.map(line =&gt; &#123;</span><br><span class="line">      ...</span><br><span class="line">      <span class="type">Row</span>(id, name, age, fv)</span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure>

<p>4.创建StructType（schema）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//结果类型，其实就是表头，用于描述DataFrame</span></span><br><span class="line"><span class="keyword">val</span> sch: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">List</span>(</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"id"</span>, <span class="type">LongType</span>, <span class="literal">true</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"name"</span>, <span class="type">StringType</span>, <span class="literal">true</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"age"</span>, <span class="type">IntegerType</span>, <span class="literal">true</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">"fv"</span>, <span class="type">DoubleType</span>, <span class="literal">true</span>)</span><br><span class="line">))</span><br></pre></td></tr></table></figure>

<p>5.通过rowRDD和schema创建DataFrame （使用sqlContext方法，不用导入隐式转换）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//将RowRDD关联schem</span></span><br><span class="line"><span class="keyword">val</span> bdf: <span class="type">DataFrame</span> = sqlContext.createDataFrame(rowRDD, sch)</span><br></pre></td></tr></table></figure>

<p>6.使用DSL风格操作数据（dataFrameAPI操作数据）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//不使用SQL的方式，就不用注册临时表了</span></span><br><span class="line"><span class="keyword">val</span> df1: <span class="type">DataFrame</span> = bdf.select(<span class="string">"name"</span>, <span class="string">"age"</span>, <span class="string">"fv"</span>)</span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"><span class="keyword">val</span> df2: <span class="type">DataFrame</span> = df1.orderBy($<span class="string">"fv"</span> desc, $<span class="string">"age"</span> asc)</span><br></pre></td></tr></table></figure>

<p>//6.将DataFrame注册成临时表（SQL方式）<br>//7.书写SQL（Transformation）<br>8.执行Action</p>
<h1 id="导入隐式转换的地方："><a href="#导入隐式转换的地方：" class="headerlink" title="导入隐式转换的地方："></a>导入隐式转换的地方：</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"><span class="keyword">val</span> bdf: <span class="type">DataFrame</span> = boyRDD.toDF</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sparkSession.implicits._</span><br><span class="line"><span class="keyword">val</span> df2: <span class="type">Dataset</span>[<span class="type">Row</span>] = df.where($<span class="string">"fv"</span> &gt; <span class="number">98</span>).orderBy($<span class="string">"fv"</span> desc, $<span class="string">"age"</span> asc)</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> lines: <span class="type">Dataset</span>[<span class="type">String</span>] = spark.createDataset(<span class="type">List</span>(<span class="string">"1,laozhoa,china"</span>, <span class="string">"2,laoduan,usa"</span>, <span class="string">"3,laoyang,jp"</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> range: <span class="type">Dataset</span>[<span class="type">Long</span>] = spark.range(<span class="number">1</span>, <span class="number">11</span>)</span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> result = range.groupBy().agg(geomean($<span class="string">"id"</span>).as(<span class="string">"geomean"</span>)) <span class="comment">//这里可以不用掉groupBy</span></span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._ <span class="comment">//dataSet调用map方法用到了隐式转换</span></span><br><span class="line"><span class="keyword">val</span> rulesLines:<span class="type">Dataset</span>[<span class="type">String</span>] = spark.read.textFile(args(<span class="number">0</span>))</span><br><span class="line"><span class="comment">//整理ip规则数据()</span></span><br><span class="line"><span class="keyword">val</span> ruleDataFrame: <span class="type">DataFrame</span> = rulesLines.map(line =&gt; &#123;...&#125;)</span><br></pre></td></tr></table></figure>


          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/24/SPARK学习笔记/sparkRDD笔记/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chen Bin">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="cilibili">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/24/SPARK学习笔记/sparkRDD笔记/" itemprop="url">SPARK RDD</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-07-24T11:07:20+08:00">
                2019-07-24
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="SPARK-RDD"><a href="#SPARK-RDD" class="headerlink" title="SPARK RDD"></a>SPARK RDD</h1><h2 id="day3-最受欢迎老师"><a href="#day3-最受欢迎老师" class="headerlink" title="day3 最受欢迎老师"></a>day3 最受欢迎老师</h2><p>map方法可以返回元组类型</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sbjectTeacherAndOne: <span class="type">RDD</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)] = lines.map(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> index = line.lastIndexOf(<span class="string">"/"</span>)</span><br><span class="line">      <span class="keyword">val</span> teacher = line.substring(index + <span class="number">1</span>)</span><br><span class="line">      <span class="keyword">val</span> httpHost = line.substring(<span class="number">0</span>, index)</span><br><span class="line">      <span class="keyword">val</span> subject = <span class="keyword">new</span> <span class="type">URL</span>(httpHost).getHost.split(<span class="string">"[.]"</span>)(<span class="number">0</span>) <span class="comment">//“.”是特殊字符，要用[]</span></span><br><span class="line">      ((subject, teacher), <span class="number">1</span>) <span class="comment">//学科和老师联合当作key</span></span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure>

<p>下划线的操作</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> map: <span class="type">RDD</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)] = sbjectAndteacher.map((_, <span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> reduced: <span class="type">RDD</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)] = sbjectTeacherAndOne.reduceByKey(_+_)</span><br></pre></td></tr></table></figure>

<p><strong>排序操作的比较</strong>：</p>
<ol>
<li>变为scala集合进行排序：</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//.toList方法就是将RDD中的数据以scala集合存放进内存，用scala方法进行排序</span></span><br><span class="line"><span class="keyword">val</span> sorted = grouped.mapValues(_.toList.sortBy(_._2).reverse.take(topN))</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>过滤出不同学科进行操作，调用RDD的sortBy方法</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> subjects = <span class="type">Array</span>(<span class="string">"bigdata"</span>, <span class="string">"javaee"</span>, <span class="string">"php"</span>)</span><br><span class="line"><span class="keyword">for</span> (sb &lt;- subjects) &#123;</span><br><span class="line">      <span class="comment">//该RDD中对应的数据仅有一个学科的数据（因为过滤过了）</span></span><br><span class="line">      <span class="keyword">val</span> filtered: <span class="type">RDD</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)] = reduced.filter(_._1._1 == sb)</span><br><span class="line">      <span class="comment">//现在调用的是RDD的sortBy方法，(take是一个action，会触发任务提交)</span></span><br><span class="line">      <span class="keyword">val</span> favTeacher = filtered.sortBy(_._2, <span class="literal">false</span>).take(topN)</span><br><span class="line">      <span class="comment">//打印 ==必须要加上toBuffer==</span></span><br><span class="line">      println(favTeacher.toBuffer)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<ol start="3">
<li><strong>自定义分区</strong>：将相同学科的数据利用自定义分区规则，到同一个分区</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//聚合，将学科和老师联合当做key</span></span><br><span class="line"><span class="keyword">val</span> reduced: <span class="type">RDD</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)] = sbjectTeacherAndOne.reduceByKey(_+_)</span><br><span class="line"><span class="comment">//计算有多少学科</span></span><br><span class="line"><span class="keyword">val</span> subjects: <span class="type">Array</span>[<span class="type">String</span>] = reduced.map(_._1._1).distinct().collect()</span><br><span class="line"><span class="comment">//自定义一个分区器，并且按照指定的分区器进行分区</span></span><br><span class="line"><span class="keyword">val</span> sbPatitioner = <span class="keyword">new</span> <span class="type">SubjectParitioner</span>(subjects);</span><br><span class="line"><span class="comment">//partitionBy按照指定的分区规则进行分区</span></span><br><span class="line"><span class="comment">//调用partitionBy时RDD的Key是(String, String)，因为类型为reduced: RDD[((String, String), Int)]</span></span><br><span class="line"><span class="keyword">val</span> partitioned: <span class="type">RDD</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)] = reduced.partitionBy(sbPatitioner)</span><br><span class="line"><span class="comment">//如果一次拿出一个分区(可以操作一个分区中的数据了)</span></span><br><span class="line"><span class="keyword">val</span> sorted: <span class="type">RDD</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)] = partitioned.mapPartitions(it =&gt; &#123;</span><br><span class="line">	<span class="comment">//将迭代器（里的数据）转换成list，然后排序，在转换成迭代器返回</span></span><br><span class="line">	it.toList.sortBy(_._2).reverse.take(topN).iterator</span><br><span class="line">	&#125;)</span><br><span class="line"><span class="keyword">val</span> r: <span class="type">Array</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)] = sorted.collect()</span><br><span class="line">println(r.toBuffer)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">自定义分区器</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SubjectParitioner</span>(<span class="params">sbs: <span class="type">Array</span>[<span class="type">String</span>]</span>) <span class="keyword">extends</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line">  <span class="comment">//相当于主构造器（new的时候回执行一次）</span></span><br><span class="line">  <span class="comment">//用于存放规则的一个map</span></span><br><span class="line">  <span class="keyword">val</span> rules = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">Int</span>]()</span><br><span class="line">  <span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span>(sb &lt;- sbs) &#123;</span><br><span class="line">    <span class="comment">//rules(sb) = i</span></span><br><span class="line">    rules.put(sb, i)</span><br><span class="line">    i += <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//返回分区的数量（下一个RDD有多少分区）</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = sbs.length</span><br><span class="line">  <span class="comment">//根据传入的key计算分区标号</span></span><br><span class="line">  <span class="comment">//key是一个元组（String， String）</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="comment">//获取学科名称  强转为RDD的[(String, String)]类型</span></span><br><span class="line">    <span class="keyword">val</span> subject = key.asInstanceOf[(<span class="type">String</span>, <span class="type">String</span>)]._1</span><br><span class="line">    <span class="comment">//根据规则计算分区编号</span></span><br><span class="line">    rules(subject)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol start="4">
<li><p>对自定义分区器改进：shuffle次数减少</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> reduced: <span class="type">RDD</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)] = s..e.reduceByKey(sbPatitioner, _+_)</span><br><span class="line"><span class="comment">//直接在reduceByKey里传入一个分区器，整合了上面的reduceBykey和partitionBy两次shuffle</span></span><br></pre></td></tr></table></figure>



</li>
</ol>
<h2 id="day4-求ip归属地"><a href="#day4-求ip归属地" class="headerlink" title="day4 求ip归属地"></a>day4 求ip归属地</h2><ul>
<li><h4 id="读取文件内容方法：（以内存方式存数据）"><a href="#读取文件内容方法：（以内存方式存数据）" class="headerlink" title="读取文件内容方法：（以内存方式存数据）"></a>读取文件内容方法：（以内存方式存数据）</h4></li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readRules</span></span>(path: <span class="type">String</span>): <span class="type">Array</span>[(<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">String</span>)] = &#123;</span><br><span class="line">    <span class="comment">//读取ip规则</span></span><br><span class="line">    <span class="keyword">val</span> bf: <span class="type">BufferedSource</span> = <span class="type">Source</span>.fromFile(path)</span><br><span class="line">    <span class="keyword">val</span> lines: <span class="type">Iterator</span>[<span class="type">String</span>] = bf.getLines()</span><br><span class="line">    <span class="comment">//对ip规则进行整理，并放入到内存</span></span><br><span class="line">    <span class="keyword">val</span> rules: <span class="type">Array</span>[(<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">String</span>)] = lines.map(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> fileds = line.split(<span class="string">"[|]"</span>)</span><br><span class="line">      <span class="keyword">val</span> startNum = fileds(<span class="number">2</span>).toLong</span><br><span class="line">      <span class="keyword">val</span> endNum = fileds(<span class="number">3</span>).toLong</span><br><span class="line">      <span class="keyword">val</span> province = fileds(<span class="number">6</span>)</span><br><span class="line">      (startNum, endNum, province)</span><br><span class="line">    &#125;).toArray</span><br><span class="line">    rules <span class="comment">//返回一个Array，调用这个方法时Array数据存放在内存里</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>读取文件内容方法：（<strong>以RDD形式读取数据</strong>）</p>
<p>分布式读取处理数据，用collect方法把所有数据收集起来</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//取到HDFS中的ip规则</span></span><br><span class="line"><span class="keyword">val</span> rulesLines:<span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(args(<span class="number">0</span>))</span><br><span class="line"><span class="comment">//整理ip规则数据</span></span><br><span class="line"><span class="keyword">val</span> ipRulesRDD: <span class="type">RDD</span>[(<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">String</span>)] = rulesLines.map(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> fields = line.split(<span class="string">"[|]"</span>)</span><br><span class="line">      <span class="keyword">val</span> startNum = fields(<span class="number">2</span>).toLong</span><br><span class="line">      <span class="keyword">val</span> endNum = fields(<span class="number">3</span>).toLong</span><br><span class="line">      <span class="keyword">val</span> province = fields(<span class="number">6</span>)</span><br><span class="line">      (startNum, endNum, province)</span><br><span class="line">    &#125;)</span><br><span class="line"><span class="comment">//将分散在多个Executor中的部分IP规则收集到Driver端</span></span><br><span class="line"><span class="keyword">val</span> rulesInDriver: <span class="type">Array</span>[(<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">String</span>)] = ipRulesRDD.collect()</span><br><span class="line"></span><br><span class="line"><span class="comment">//将Driver端的数据广播到Executor</span></span><br><span class="line"><span class="comment">//广播变量的引用（还在Driver端）</span></span><br><span class="line"><span class="keyword">val</span> broadcastRef: <span class="type">Broadcast</span>[<span class="type">Array</span>[(<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">String</span>)]] = sc.broadcast(rulesInDriver)</span><br></pre></td></tr></table></figure>



</li>
</ul>
<p>  处理文件逻辑：</p>
  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//在Driver端获取到全部的IP规则数据（全部的IP规则数据在某一台机器上，跟Driver在同一台机器上）</span></span><br><span class="line"><span class="comment">//全部的IP规则在Driver端了（在Driver端的内存中了）</span></span><br><span class="line"><span class="keyword">val</span> rules: <span class="type">Array</span>[(<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">String</span>)] = <span class="type">MyUtils</span>.readRules(args(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">//将Drive端的数据广播到Executor中</span></span><br><span class="line"><span class="comment">//调用sc上的广播方法</span></span><br><span class="line"><span class="comment">//广播变量的引用（还在Driver端）</span></span><br><span class="line"><span class="keyword">val</span> broadcastRef: <span class="type">Broadcast</span>[<span class="type">Array</span>[(<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">String</span>)]] = sc.broadcast(rules)</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建RDD，读取访问日志</span></span><br><span class="line"><span class="keyword">val</span> accessLines: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(args(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">//这个函数是在哪一端定义的？（Driver）</span></span><br><span class="line"><span class="keyword">val</span> func = (line: <span class="type">String</span>) =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> fields = line.split(<span class="string">"[|]"</span>)</span><br><span class="line">    <span class="keyword">val</span> ip = fields(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">//将ip转换成十进制</span></span><br><span class="line">    <span class="keyword">val</span> ipNum = <span class="type">MyUtils</span>.ip2Long(ip)</span><br><span class="line">    <span class="comment">//进行二分法查找，通过Driver端的引用或取到Executor中的广播变量</span></span><br><span class="line">    <span class="comment">//（该函数中的代码是在Executor中别调用执行的，通过广播变量的引用，就可以拿到当前Executor中的广播的规则了），Task是在Driver端生成的，广播变量的引用是伴随着Task被发送到Executor中的</span></span><br><span class="line">    <span class="keyword">val</span> rulesInExecutor: <span class="type">Array</span>[(<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">String</span>)] = broadcastRef.value</span><br><span class="line">    <span class="comment">//查找</span></span><br><span class="line">    <span class="keyword">var</span> province = <span class="string">"未知"</span></span><br><span class="line">    <span class="keyword">val</span> index = <span class="type">MyUtils</span>.binarySearch(rulesInExecutor, ipNum)</span><br><span class="line">    <span class="keyword">if</span> (index != <span class="number">-1</span>) &#123;</span><br><span class="line">			province = rulesInExecutor(index)._3</span><br><span class="line">        &#125;</span><br><span class="line">			(province, <span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line"><span class="comment">//整理数据</span></span><br><span class="line"><span class="keyword">val</span> proviceAndOne: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = accessLines.map(func)</span><br><span class="line"><span class="keyword">val</span> reduced: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = proviceAndOne.reduceByKey(_+_)</span><br><span class="line"><span class="comment">//将结果打印</span></span><br><span class="line"><span class="keyword">val</span> r = reduced.collect()</span><br></pre></td></tr></table></figure>

<ul>
<li><p><strong>连接JDBC，将数据写入数据库：</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//一次拿出一个分区(一个分区用一个连接，可以将一个分区中的多条数据写完在释放jdbc连接，这样更节省资源) 这是一个action</span></span><br><span class="line">reduced.foreachPartition(it =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> conn: <span class="type">Connection</span> = <span class="type">DriverManager</span>.getConnection(<span class="string">"jdbc:mysql://localhost:3306/bigdata?characterEncoding=UTF-8"</span>, <span class="string">"root"</span>, <span class="string">"123568"</span>)</span><br><span class="line">    </span><br><span class="line">  <span class="comment">//将数据通过Connection写入到数据库</span></span><br><span class="line">  <span class="keyword">val</span> pstm: <span class="type">PreparedStatement</span> = conn.prepareStatement(<span class="string">"INSERT INTO access_log VALUES (?, ?)"</span>)</span><br><span class="line">  <span class="comment">//将一个分区中的每一条数据拿出来</span></span><br><span class="line">  it.foreach(tp =&gt; &#123;</span><br><span class="line">    pstm.setString(<span class="number">1</span>, tp._1)</span><br><span class="line">    pstm.setInt(<span class="number">2</span>, tp._2)</span><br><span class="line">    pstm.executeUpdate()</span><br><span class="line">  &#125;)</span><br><span class="line">  pstm.close()</span><br><span class="line">  conn.close()</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<p><strong>改进：将匿名内部类写成方法传入参数：</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data2MySQL</span></span>(it: <span class="type">Iterator</span>[(<span class="type">String</span>, <span class="type">Int</span>)]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">//一个迭代器代表一个分区，分区中有多条数据</span></span><br><span class="line">  <span class="comment">//先获得一个JDBC连接</span></span><br><span class="line">  <span class="keyword">val</span> conn: <span class="type">Connection</span> = <span class="type">DriverManager</span>.getConnection(<span class="string">"jdbc:mysql://..."</span>)</span><br><span class="line">  <span class="comment">//将数据通过Connection写入到数据库</span></span><br><span class="line">  <span class="keyword">val</span> pstm: <span class="type">PreparedStatement</span> = conn.prepareStatement(<span class="string">"INSERT INTO .. VALUES (?, ?)"</span>)</span><br><span class="line">  <span class="comment">//将分区中的数据一条一条写入到MySQL中</span></span><br><span class="line">  it.foreach(tp =&gt; &#123;</span><br><span class="line">    pstm.setString(<span class="number">1</span>, tp._1)</span><br><span class="line">    pstm.setInt(<span class="number">2</span>, tp._2)</span><br><span class="line">    pstm.executeUpdate()</span><br><span class="line">  &#125;)</span><br><span class="line">  <span class="comment">//将分区中的数据全部写完之后，在关闭连接</span></span><br><span class="line">  <span class="keyword">if</span>(pstm != <span class="literal">null</span>) &#123;</span><br><span class="line">    pstm.close()</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (conn != <span class="literal">null</span>) &#123;</span><br><span class="line">    conn.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>方法调用：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">reduced.foreachPartition(it =&gt; <span class="type">MyUtils</span>.data2MySQL(it))<span class="comment">//方法一</span></span><br><span class="line">reduced.foreachPartition(<span class="type">MyUtils</span>.data2MySQL() _)<span class="comment">//方法二 下划线将方法转为函数</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
<ul>
<li><h4 id="JdbcRDD"><a href="#JdbcRDD" class="headerlink" title="JdbcRDD"></a>JdbcRDD</h4></li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">JdbcRddDemo</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> getConn = () =&gt; &#123;</span><br><span class="line">    <span class="type">DriverManager</span>.getConnection(<span class="string">"jdbc:mysql://localhost:3306/bigdata?characterEncoding=UTF-8"</span>, <span class="string">"root"</span>, <span class="string">"123568"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"JdbcRddDemo"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="comment">//创建RDD，这个RDD会记录以后从MySQL中读数据</span></span><br><span class="line">    <span class="comment">//new 了RDD，里面没有真正要计算的数据，而是告诉这个RDD，以后触发Action时到哪里读取数据</span></span><br><span class="line">    <span class="keyword">val</span> jdbcRDD: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>, <span class="type">Int</span>)] = <span class="keyword">new</span> <span class="type">JdbcRDD</span>(</span><br><span class="line">      sc, <span class="comment">//sparkContext</span></span><br><span class="line">      getConn,</span><br><span class="line">      <span class="string">"SELECT * FROM logs WHERE id &gt;= ? AND id &lt;= ?"</span>, </span><br><span class="line">      <span class="comment">//注意！：这里必须有等于，不然下一个分区会少包含id 后面的那个数</span></span><br><span class="line">      <span class="number">1</span>,</span><br><span class="line">      <span class="number">5</span>,</span><br><span class="line">      <span class="number">2</span>, <span class="comment">//分区数量</span></span><br><span class="line">      rs =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> id = rs.getInt(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">val</span> name = rs.getString(<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">val</span> age = rs.getInt(<span class="number">3</span>)</span><br><span class="line">        (id, name, age)</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">//触发Action</span></span><br><span class="line">    <span class="keyword">val</span> r = jdbcRDD.collect()</span><br><span class="line">    println(r.toBuffer)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>注意！</strong>：这里必须有等于，不然下一个分区会少包含id 后面的那个数</p>
<ul>
<li><h4 id="cache与checkpoint"><a href="#cache与checkpoint" class="headerlink" title="cache与checkpoint"></a>cache与checkpoint</h4><h5 id="cache"><a href="#cache" class="headerlink" title="cache"></a>cache</h5><ul>
<li>cache方法，没有生成新的RDD，也没有触发任务执行，只会标记该RDD分区对应的数据（第一次触发Action时）放入到内存</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> cached = reduced.cache()</span><br><span class="line">cached.unpersisted(<span class="literal">true</span>)<span class="comment">//释放cached占用的内存，true表示非阻塞式释放内存</span></span><br></pre></td></tr></table></figure>

<ul>
<li>什么时候进行cache<pre><code>1.要求的计算速度快
  2.集群的资源要足够大
  3.重要：cache的数据会多次的触发Action
  4.先进行过滤，然后将缩小范围的数据在cache到内存</code></pre></li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//第一个参数，放到磁盘</span></span><br><span class="line"><span class="comment">//第二个参数，放到内存</span></span><br><span class="line"><span class="comment">//第三个参数，磁盘中的数据，不是以java对象的方式保存</span></span><br><span class="line"><span class="comment">//第四个参数，内存中的数据，以java对象的方式保存</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">MEMORY_AND_DISK</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br></pre></td></tr></table></figure>

<h5 id="checkpoint"><a href="#checkpoint" class="headerlink" title="checkpoint"></a>checkpoint</h5><ul>
<li>checkpoint方法，没有生成新的RDD，也是没有触发Action，也是标记以后触发Action时会将数据保存到HDFS中</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sc.setCheckpointDir(<span class="string">"hdfs://node-4:9000/ck"</span>)</span><br><span class="line">reduced.checkpoint()<span class="comment">//将reduced中的数据保存在checkpoint中，checkpoint改变了RDD的继承关系</span></span><br></pre></td></tr></table></figure>

<ul>
<li>什么时候做checkpoint<pre><code>1.迭代计算，要求保证数据安全
  2.对速度要求不高（跟cache到内存进行对比）
  3.将中间结果保存到hdfs</code></pre></li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//设置checkpoint目录（分布式文件系统的目录hdfs目录）</span></span><br><span class="line"><span class="comment">//经过复杂进行，得到中间结果</span></span><br><span class="line"><span class="comment">//将中间结果checkpoint到指定的hdfs目录</span></span><br><span class="line"><span class="comment">//后续的计算，就可以使用前面ck的数据了</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
<h2 id="day5-自定义排序和切分Stage"><a href="#day5-自定义排序和切分Stage" class="headerlink" title="day5 自定义排序和切分Stage"></a>day5 自定义排序和切分Stage</h2><h3 id="1-自定义排序"><a href="#1-自定义排序" class="headerlink" title="1 自定义排序"></a>1 自定义排序</h3><ol>
<li>在一个类里实现ordered或者ordering接口，将要处理的数据封装成RDD</li>
</ol>
<p>实现ordered接口的类：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">val name: <span class="type">String</span>, val age: <span class="type">Int</span>, val fv: <span class="type">Int</span></span>) </span></span><br><span class="line"><span class="class"><span class="keyword">extends</span> <span class="title">Ordered</span>[<span class="type">User</span>] <span class="keyword">with</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="comment">//实现序列化，因为在shuffle过程中封装数据的User要序列化，需要在网络传输</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(that: <span class="type">User</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="keyword">this</span>.fv == that.fv) &#123;</span><br><span class="line">      <span class="keyword">this</span>.age - that.age</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      -(<span class="keyword">this</span>.fv - that.fv)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>: <span class="type">String</span> = <span class="string">s"name: <span class="subst">$name</span>, age: <span class="subst">$age</span>, fv: <span class="subst">$fv</span>"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>调用方法：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//切分整理数据</span></span><br><span class="line"><span class="keyword">val</span> userRDD: <span class="type">RDD</span>[<span class="type">User</span>] = lines.map(line =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> fields = line.split(<span class="string">" "</span>)</span><br><span class="line">  <span class="keyword">val</span> name = fields(<span class="number">0</span>)</span><br><span class="line">  <span class="keyword">val</span> age = fields(<span class="number">1</span>).toInt</span><br><span class="line">  <span class="keyword">val</span> fv = fields(<span class="number">2</span>).toInt</span><br><span class="line">  <span class="comment">//(name, age, fv)</span></span><br><span class="line">  <span class="keyword">new</span> <span class="type">User</span>(name, age, fv)</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">//将RDD里面装的User类型的数据进行排序</span></span><br><span class="line"><span class="keyword">val</span> sorted: <span class="type">RDD</span>[<span class="type">User</span>] = userRDD.sortBy(u =&gt; u) <span class="comment">//传入参数就是u，u就是匹配规则</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li>在类里实现ordered或者ordering接口（类里只包含需要参与排序的属性），将要处理的数据封装成tupple</li>
</ol>
<p>只含排序参数的类：</p>
  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Boy</span>(<span class="params">val age: <span class="type">Int</span>, val fv: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Ordered</span>[<span class="type">Boy</span>] <span class="keyword">with</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(that: <span class="type">Boy</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="keyword">this</span>.fv == that.fv) &#123;</span><br><span class="line">      <span class="keyword">this</span>.age - that.age</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      -(<span class="keyword">this</span>.fv - that.fv)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>   调用方法：数据整理成tupple，排序逻辑传入排序类的对象</p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//切分整理数据</span></span><br><span class="line"><span class="keyword">val</span> tpRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>, <span class="type">Int</span>)] = lines.map(line =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> fields = line.split(<span class="string">" "</span>)</span><br><span class="line">    <span class="keyword">val</span> name = fields(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> age = fields(<span class="number">1</span>).toInt</span><br><span class="line">    <span class="keyword">val</span> fv = fields(<span class="number">2</span>).toInt</span><br><span class="line">    (name, age, fv)</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//排序(传入了一个排序规则，不会改变数据的格式，只会改变顺序)</span></span><br><span class="line">    <span class="keyword">val</span> sorted: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>, <span class="type">Int</span>)] = tpRDD.sortBy(tp =&gt; <span class="keyword">new</span> <span class="type">Boy</span>(tp._2, tp._3))</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>利用模式匹配类（case class父类实现了Serializable接口）,也不用new 对象，但是对象也是多例的</li>
</ol>
<p>模式匹配类：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Man</span>(<span class="params">age: <span class="type">Int</span>, fv: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Ordered</span>[<span class="type">Man</span>] </span>&#123; <span class="comment">//参数都是val类型</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(that: <span class="type">Man</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="keyword">this</span>.fv == that.fv) &#123;</span><br><span class="line">      <span class="keyword">this</span>.age - that.age</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      -(<span class="keyword">this</span>.fv - that.fv)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>调用方法：不用显示地去new对象，但对象还是多例的</p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">...前面相同</span><br><span class="line"><span class="keyword">val</span> sorted: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>, <span class="type">Int</span>)] = tpRDD.sortBy(tp =&gt; <span class="type">Man</span>(tp._2, tp._3))</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>传入sortBy方法的一个隐式参数</li>
</ol>
<p>有隐式参数的类：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SortRules</span> </span>&#123;</span><br><span class="line">  <span class="keyword">implicit</span> <span class="class"><span class="keyword">object</span> <span class="title">OrderingXiaoRou</span> <span class="keyword">extends</span> <span class="title">Ordering</span>[<span class="type">XianRou</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: <span class="type">XianRou</span>, y: <span class="type">XianRou</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">      <span class="keyword">if</span>(x.fv == y.fv) &#123;</span><br><span class="line">        x.age - y.age</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        y.fv - x.fv</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>调用方法：导入隐式参数方法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//排序(传入了一个排序规则，不会改变数据的格式，只会改变顺序)</span></span><br><span class="line"><span class="keyword">import</span> <span class="type">SortRules</span>.<span class="type">OrderingXiaoRou</span></span><br><span class="line"><span class="keyword">val</span> sorted: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>, <span class="type">Int</span>)] = tpRDD.sortBy(tp =&gt; <span class="type">XianRou</span>(tp._2, tp._3))</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">XianRou</span>(<span class="params">age: <span class="type">Int</span>, fv: <span class="type">Int</span></span>)<span class="title">//只定义这个类</span></span></span><br></pre></td></tr></table></figure>

<ol start="5">
<li>利用tupple的比较规则：</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">...前面相同</span><br><span class="line"><span class="comment">//充分利用元组的比较规则，元组的比较规则：先比第一，相等再比第二个</span></span><br><span class="line"><span class="keyword">val</span> sorted: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>, <span class="type">Int</span>)] = tpRDD.sortBy(tp =&gt; (-tp._3, tp._2))</span><br></pre></td></tr></table></figure>

<ol start="6">
<li>利用tupple的比较规则2</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Ordering[(Int, Int)]最终比较的规则格式</span></span><br><span class="line"><span class="comment">//on[(String, Int, Int)]未比较之前的数据格式</span></span><br><span class="line"><span class="comment">//(t =&gt;(-t._3, t._2))怎样将规则转换成想要比较的格式</span></span><br><span class="line"><span class="keyword">implicit</span> <span class="keyword">val</span> rules = <span class="type">Ordering</span>[(<span class="type">Int</span>, <span class="type">Int</span>)].on[(<span class="type">String</span>, <span class="type">Int</span>, <span class="type">Int</span>)](t =&gt;(-t._3, t._2))</span><br><span class="line"><span class="keyword">val</span> sorted: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>, <span class="type">Int</span>)] = tpRDD.sortBy(tp =&gt; tp)</span><br><span class="line">备注：有隐式参数的比较规则时会先调用隐式参数比较规则</span><br></pre></td></tr></table></figure>

<h3 id="2-切分Stage"><a href="#2-切分Stage" class="headerlink" title="2 切分Stage"></a>2 切分Stage</h3><ul>
<li>四个步骤</li>
</ul>
<ol>
<li><p>构建DAG（调用RDD上的方法）</p>
</li>
<li><p>DAGScheduler将DAG切分Stage（切分的依据是Shuffle），将Stage中生成的Task以TaskSet的形式给TaskScheduler</p>
</li>
<li><p>TaskScheduler调度Task（根据资源情况将Task调度到相应的Executor中）</p>
</li>
<li><p>Executor接收Task，然后将Task丢入到线程池中执行</p>
</li>
</ol>
<ul>
<li>为什么要切分Stage？<pre><code>一个复杂的业务逻辑（将多台机器上具有相同属性的数据聚合到一台机器上：shuffle）
  如果有shuffle，那么就意味着前面阶段产生的结果后，才能执行下一个阶段，下一个阶段的计算要依赖上一个阶段的数据。
  在同一个Stage中，会有多个算子，可以合并在一起，我们称其为pipeline（流水线：严格按照流程、顺序执行）</code></pre></li>
</ul>
<p><img src="sparkRDD%E7%AC%94%E8%AE%B0%5C%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B.png" alt="执行过程"></p>
<h2 id="day6-序列化及多线程问题"><a href="#day6-序列化及多线程问题" class="headerlink" title="day6 序列化及多线程问题"></a>day6 序列化及多线程问题</h2><h3 id="1-序列化问题"><a href="#1-序列化问题" class="headerlink" title="1 序列化问题"></a>1 序列化问题</h3><ul>
<li><p>方法一：在map方法里new 对象，在每个Task里面类加载，每map一条数据就会生成对象</p>
<p>​        缺点：非常浪费资源</p>
</li>
<li><p>方法二：在map方法前先new对象，map操作使用这个对象的引用。但是对象的类需要序列化</p>
<p>​        缺点：在一个Excutor里的每个Task都会反序列化出不同的对象</p>
</li>
<li><p>方法三：将类定义为Object，Excutor在加载类的时候只会加载一次，在一个Excutor里的所有Task都会共用这个”静态类(Object)“里的属性和方法</p>
<p>​        缺点：Object在Driver端生成，通过网络传输到各个Excutor占时间</p>
</li>
<li><p>方法四：只在Map方法里使用Object的属性或者方法。可以不用网络传输对象了，直接在Excutor里加载类</p>
</li>
</ul>
<p>定义的规则类：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Rules</span> </span>&#123;									------------------------&gt;方法一</span><br><span class="line">  <span class="keyword">val</span> rulesMap = <span class="type">Map</span>(<span class="string">"hadoop"</span> -&gt; <span class="number">2.7</span>, <span class="string">"spark"</span> -&gt; <span class="number">2.2</span>)</span><br><span class="line">  <span class="keyword">val</span> hostname = <span class="type">InetAddress</span>.getLocalHost.getHostName</span><br><span class="line">  println(hostname + <span class="string">"@@@@@@@@@@@@@@@@"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Rules</span> <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;				------------------------&gt;方法二</span><br><span class="line">  <span class="keyword">val</span> rulesMap = <span class="type">Map</span>(<span class="string">"hadoop"</span> -&gt; <span class="number">2.7</span>, <span class="string">"spark"</span> -&gt; <span class="number">2.2</span>)</span><br><span class="line">  <span class="keyword">val</span> hostname = <span class="type">InetAddress</span>.getLocalHost.getHostName</span><br><span class="line">  println(hostname + <span class="string">"@@@@@@@@@@@@@@@@"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Rules</span> <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;				------------------------&gt;方法三</span><br><span class="line">  <span class="keyword">val</span> rulesMap = <span class="type">Map</span>(<span class="string">"hadoop"</span> -&gt; <span class="number">2.7</span>, <span class="string">"spark"</span> -&gt; <span class="number">2.2</span>)</span><br><span class="line">  <span class="keyword">val</span> hostname = <span class="type">InetAddress</span>.getLocalHost.getHostName</span><br><span class="line">  println(hostname + <span class="string">"@@@@@@@@@@@@@@@@"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//第三种方式，希望Rules在EXecutor中被初始化（不走网络了，就不必实现序列化接口）</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Rules</span> </span>&#123;									------------------------&gt;方法四</span><br><span class="line">  <span class="keyword">val</span> rulesMap = <span class="type">Map</span>(<span class="string">"hadoop"</span> -&gt; <span class="number">2.7</span>, <span class="string">"spark"</span> -&gt; <span class="number">2.2</span>)</span><br><span class="line">  <span class="keyword">val</span> hostname = <span class="type">InetAddress</span>.getLocalHost.getHostName</span><br><span class="line">  println(hostname + <span class="string">"@@@@@@@@@@@@@@@@"</span>)</span><br><span class="line">  </span><br><span class="line">  备注： 这里pritln方法就相当于静态代码块，在类加载的时候会执行。</span><br><span class="line">  		可以利用这个方法来实现读取共享数据等操作。</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>调用方法：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SerTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//在Driver端被实例化		</span></span><br><span class="line">    <span class="comment">//val rules = new Rules					------------------------&gt;方法二</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//初始化object（在Driver端）</span></span><br><span class="line">    <span class="comment">//var rules = Rules						------------------------&gt;方法三</span></span><br><span class="line">    <span class="comment">//println("@@@@@@@@@@@@" + rules.toString + "@@@@@@@@@@@@")</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"SerTest"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> lines: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(args(<span class="number">0</span>))</span><br><span class="line">    <span class="keyword">val</span> r = lines.map(word =&gt; &#123;</span><br><span class="line">      <span class="comment">//在map的函数中，创建一个rules实例(太浪费资源)</span></span><br><span class="line">      <span class="comment">//val rules = new Rules 				------------------------&gt;方法一</span></span><br><span class="line">      <span class="comment">//函数的执行是在Executor执行的（Task中执行的）</span></span><br><span class="line">      <span class="keyword">val</span> hostname = <span class="type">InetAddress</span>.getLocalHost.getHostName</span><br><span class="line">      <span class="keyword">val</span> threadName = <span class="type">Thread</span>.currentThread().getName</span><br><span class="line">      <span class="comment">//rules的实际是在Executor中使用的</span></span><br><span class="line">      (hostname, threadName, <span class="type">Rules</span>.rulesMap.getOrElse(word, <span class="number">0</span>), <span class="type">Rules</span>.toString)</span><br><span class="line">    &#125;)</span><br><span class="line">      <span class="comment">//在map方法里用到Rules的引用rules			------------------------&gt;方法一~三</span></span><br><span class="line">      <span class="comment">//只在map方法里用到Rules(Object)的属性或方法------------------------&gt;方法四</span></span><br><span class="line">    r.saveAsTextFile(args(<span class="number">1</span>))</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="2-多线程问题"><a href="#2-多线程问题" class="headerlink" title="2 多线程问题"></a>2 多线程问题</h3><p>以SimpleDateFormat格式化处理日志中时间为例</p>
<p><strong>多线程问题：</strong> 如果object使用了成员变量（且这个成员变量是线程不安全的），那么会出现线程安全问题，因为object是一个单例，多线程(一个Excutor里多个Task)可以同时调用这个方法。</p>
<p>时间处理工具类：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">FilterUtilsV4</span></span>&#123;</span><br><span class="line">  <span class="comment">//如果object使用了成员变量，那么会出现线程安全问题，因为object是一个单例，多线程(一个Excutor里多个Task)可以同时调用这个方法</span></span><br><span class="line">  <span class="keyword">val</span> dateFormat = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy年MM月dd日,E,HH:mm:ss"</span>)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">filterByTime</span></span>(fields: <span class="type">Array</span>[<span class="type">String</span>], startTime: <span class="type">Long</span>, endTime: <span class="type">Long</span>) = &#123;</span><br><span class="line">    <span class="keyword">val</span> time = fields(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> timeLong = dateFormat.parse(time).getTime</span><br><span class="line">    timeLong &gt;= startTime &amp;&amp; timeLong &lt; endTime</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>调用方法：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"GameKPI"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="comment">//整理并过滤</span></span><br><span class="line"><span class="keyword">val</span> splited: <span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">String</span>]] = lines.map(line =&gt; line.split(<span class="string">"[|]"</span>))</span><br><span class="line"><span class="keyword">val</span> filtered = splited.filter(fields =&gt; &#123;</span><br><span class="line">    <span class="comment">//如果FilterUtilsV4是一个object，把FilterUtilsV4写在函数内部，它是在Executor中被初始化的</span></span><br><span class="line">    <span class="comment">//FilterUtilsV4是在一个Executor进程中是单例的</span></span><br><span class="line">  <span class="type">FilterUtilsV4</span>.filterByTime(fields, startTime, endTime)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<ul>
<li>解决方法一：用线程安全的成员变量，或者加锁</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//FastDateFormat是线程安全的，即用线程安全的成员变量</span></span><br><span class="line"><span class="keyword">val</span> dateFormat = <span class="type">FastDateFormat</span>.getInstance(<span class="string">"yyyy年MM月dd日,E,HH:mm:ss"</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>解决方法二：将Object类改为Class类，这样每个Task都会反序列化出不同的对象</li>
</ul>
<p>时间处理工具类：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FilterUtilsV3</span></span>&#123;</span><br><span class="line">  <span class="keyword">val</span> dateFormat = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy年MM月dd日,E,HH:mm:ss"</span>)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">filterByTime</span></span>(fields: <span class="type">Array</span>[<span class="type">String</span>], startTime: <span class="type">Long</span>, endTime: <span class="type">Long</span>) = &#123;</span><br><span class="line">    <span class="keyword">val</span> time = fields(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> timeLong = dateFormat.parse(time).getTime</span><br><span class="line">    timeLong &gt;= startTime &amp;&amp; timeLong &lt; endTime</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>调用方法：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">...前面的相同</span><br><span class="line"><span class="keyword">val</span> fu = <span class="keyword">new</span> <span class="type">FilterUtilsV3</span> <span class="keyword">with</span> <span class="type">Serializable</span>  <span class="comment">//对象在new出来的时候实现序列化（匿名内部类）</span></span><br><span class="line"><span class="keyword">val</span> filtered = splited.filter(fields =&gt; &#123;</span><br><span class="line">  fu.filterByTime(fields, startTime, endTime) <span class="comment">//在这里用前面new出来的对象</span></span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<h3 id="疑惑问题："><a href="#疑惑问题：" class="headerlink" title="疑惑问题："></a>疑惑问题：</h3><p><strong>scala闭包</strong></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/22/git/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chen Bin">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="cilibili">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/22/git/" itemprop="url">常用Git操作</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-07-22T10:07:20+08:00">
                2019-07-22
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Git/" itemprop="url" rel="index">
                    <span itemprop="name">Git</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="常用Git操作"><a href="#常用Git操作" class="headerlink" title="常用Git操作"></a>常用Git操作</h1><h2 id="关联本地仓库和远程仓库"><a href="#关联本地仓库和远程仓库" class="headerlink" title="关联本地仓库和远程仓库"></a>关联本地仓库和远程仓库</h2><p>（电脑最开始可能需要生成ssh.pub，把内容在github设置里添加进去）</p>
<p>git init<br>git remote add origin <a href="https://github.com/tugenhua0707/testgit.git" target="_blank" rel="noopener">https://github.com/tugenhua0707/testgit.git</a><br>git pull origin master  –allow-unrelated-histories (在本地和远程仓库同时创建加这句)<br>git push -u origin master(第一次要用-u 以后不需要)</p>
<p><strong>关联本地分支与远程分支</strong>：</p>
<p>git checkout  –b dev origin/dev创建远程origin的dev分支到本地来<br>git branch –set -upstream dev origin/dev设置本地dev分支与远程origin/dev分支的链接</p>
<h2 id="解决冲突常用"><a href="#解决冲突常用" class="headerlink" title="解决冲突常用"></a>解决冲突常用</h2><p><strong>回退版本</strong></p>
<p>git reset –hard HDEA^^<br>git reset – hard ~100<br>git reset –hard 版本号</p>
<p><strong>工作现场与本地缓存差异</strong></p>
<p>git diff file.txt<br>git checkout –file.txt</p>
<p>git stash list<br>git stash apply/git stash drop/git stash pop</p>
<p><strong>查看提交记录</strong></p>
<p>git log 查看历史commit记录<br>git log –pretty=oneline</p>
<h2 id="合并分支"><a href="#合并分支" class="headerlink" title="合并分支"></a>合并分支</h2><p>git merge dev<br>git merge –no-ff  -m “注释” dev<br>可以保存你之前的分支历史。能够更好的查看 merge历史，以及branch 状态<a href="https://blog.csdn.net/u010940300/article/details/47419069" target="_blank" rel="noopener">https://blog.csdn.net/u010940300/article/details/47419069</a></p>
<p>git branch –d dev</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/22/mybatis注解踩坑/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chen Bin">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="cilibili">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/22/mybatis注解踩坑/" itemprop="url">mybatis获取主键注解踩坑</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-07-22T10:07:13+08:00">
                2019-07-22
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/报错记录/" itemprop="url" rel="index">
                    <span itemprop="name">报错记录</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="mybatis获取自增主键踩坑"><a href="#mybatis获取自增主键踩坑" class="headerlink" title="mybatis获取自增主键踩坑"></a>mybatis获取自增主键踩坑</h1><h2 id="第一坑（多个参数没加-Param注解）"><a href="#第一坑（多个参数没加-Param注解）" class="headerlink" title="第一坑（多个参数没加@Param注解）"></a>第一坑（多个参数没加@Param注解）</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Insert</span>(<span class="string">"insert into file_message (path, file_name, user_name) values (#&#123;path&#125;, #&#123;fileName&#125;, #&#123;userName&#125;)"</span>)</span><br><span class="line"><span class="meta">@Options</span>(useGeneratedKeys = <span class="keyword">true</span>,keyProperty = <span class="string">"id"</span>)</span><br><span class="line">Integer fileMessageSave（String path,  String fileName, String userName);</span><br></pre></td></tr></table></figure>

<p>报错：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">异常：</span><br><span class="line">org.apache.ibatis.binding.BindingException: Parameter &apos;path&apos; not found. Available</span><br><span class="line">parameters are [arg2, arg1, arg0, param3, param1, param2]</span><br></pre></td></tr></table></figure>

<p>原因：</p>
<p>传入多个参数没有加@Param注解</p>
<p>解决：</p>
<p>@Description 当mapper接口方法有多个参数时，java不会保存行参的记录， java在运行的时候会把方法中的参</p>
<p>数(long seckillId, long userPhone)变成这样:(int arg0,int arg1)， 这样我们就没有办法去传递多个参数</p>
<p><strong>注意！！</strong> 在多个参数时才会发生</p>
<h2 id="第二坑（获取自增主键-但参数不是实体类对象）"><a href="#第二坑（获取自增主键-但参数不是实体类对象）" class="headerlink" title="第二坑（获取自增主键 但参数不是实体类对象）"></a>第二坑（获取自增主键 但参数不是实体类对象）</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//添加了Param参数后</span></span><br><span class="line"><span class="meta">@Insert</span>(<span class="string">"insert into file_message (path, file_name, user_name) values (#&#123;path&#125;, #&#123;fileName&#125;, #&#123;userName&#125;)"</span>)</span><br><span class="line"><span class="meta">@Options</span>(useGeneratedKeys = <span class="keyword">true</span>,keyProperty = <span class="string">"id"</span>)</span><br><span class="line"><span class="function">Integer <span class="title">fileMessageSave</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    @Param(<span class="string">"path"</span>)</span> String path, </span></span><br><span class="line"><span class="function">    @<span class="title">Param</span><span class="params">(<span class="string">"fileName"</span>)</span> String fileName, </span></span><br><span class="line"><span class="function">    @<span class="title">Param</span><span class="params">(<span class="string">"userName"</span>)</span> String userName)</span>;</span><br></pre></td></tr></table></figure>

<p>报错：</p>
<p>无法回填主键，但可以正常插入数据</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">org.apache.ibatis.executor.ExecutorException:</span><br><span class="line">Could not determine which parameter to assign generated keys to.</span><br><span class="line">Note that when there are multiple parameters, <span class="string">'keyProperty'</span> <span class="function">must include the parameter <span class="title">name</span> <span class="params">(e.g. <span class="string">'param.id'</span>)</span>.</span></span><br><span class="line"><span class="function">Specified key properties are [id] and available parameters are [path, fileName, userName, param3, param1, param2]</span></span><br></pre></td></tr></table></figure>

<p>原因：</p>
<p>必须把主键id封装为对象属性，否则无法回填</p>
<p>解决：</p>
<p>将参数封装为对象属性，参数传入一个对象，<strong>此时就一个对象参数就不需要@Param注解了</strong></p>
<p>Tips：</p>
<p>我这里有个错误做法 <code>keyProperty = &quot;fileMessage.id&quot;</code>，导致踩坑</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Options</span>(useGeneratedKeys = <span class="keyword">true</span>,keyProperty = <span class="string">"fileMessage.id"</span>)</span><br><span class="line"><span class="function">Integer <span class="title">fileMessageSave</span><span class="params">(FileMessage fileMessage)</span></span>;</span><br></pre></td></tr></table></figure>

<h2 id="第三坑（注解keyProperty-填错）"><a href="#第三坑（注解keyProperty-填错）" class="headerlink" title="第三坑（注解keyProperty 填错）"></a>第三坑（注解keyProperty 填错）</h2><p>报错：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">org.apache.ibatis.executor.ExecutorException:</span><br><span class="line">No setter found <span class="keyword">for</span> the keyProperty <span class="string">'fileMessage.id'</span> in <span class="string">'com.example.bimfacedemo.entity.FileMessage'</span></span><br></pre></td></tr></table></figure>

<p>原因：</p>
<p>keyProperty 就直接填id就好，不要填    对象.属性</p>
<p><code>@Options(useGeneratedKeys = true,keyProperty = &quot;fileMessage.id&quot;)</code></p>
<p>受了上面报错误导( e.g. ‘param.id’)，不是写（对象.id），而是直接写id</p>
<p>解决：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Options</span>(useGeneratedKeys = <span class="keyword">true</span>, keyProperty = <span class="string">"id"</span>)</span><br></pre></td></tr></table></figure>

<h2 id="第四坑（-Insert返回值不是主键id）"><a href="#第四坑（-Insert返回值不是主键id）" class="headerlink" title="第四坑（@Insert返回值不是主键id）"></a>第四坑（@Insert返回值不是主键id）</h2><p>原因：</p>
<p>尝试调用fileMessageSave() 返回值获取自增的主键，总是得到1，不是正确的id</p>
<p>解决：</p>
<p>Integer fileMessageSave(FileMessage fileMessage);返回的是修改的条数，</p>
<p>插入后返回的自增长主键id自动回填到对象fileMessage的id属性里</p>
<p>Tips：</p>
<p>怎么通过自动回填到对象属性里的需要了解@Options(useGeneratedKeys = true,keyProperty = “id”)源码</p>
<h2 id="XML配置方式，获取自增主键"><a href="#XML配置方式，获取自增主键" class="headerlink" title="XML配置方式，获取自增主键"></a>XML配置方式，获取自增主键</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">insert</span> <span class="attr">id</span>=<span class="string">"jobSave"</span> <span class="attr">parameterType</span>=<span class="string">"com.bdai.fe.entity.Job"</span> <span class="attr">keyProperty</span>=<span class="string">"id"</span> <span class="attr">useGeneratedKeys</span>=<span class="string">"true"</span>&gt;</span></span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Chen Bin</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">categories</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/cilibili" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chen Bin</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
